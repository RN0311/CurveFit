{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Curve Fit! Background CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly. IHME COVID-19 Project For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting. Getting Started To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit make install A Note on cppad_py One of the dependencies for this package is cppad_py, a python interface for algorithmic differentiation. If you experience issues installing or importing cppad_py after doing make install , please see this page to clone and debug the build for cppad_py with setup.py . Maintainers Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu ) Jize Zhang ( jizez@uw.edu ) Alexey Sholokov ( aksh@uw.edu ) Bradley Bell ( bradbell@seanet.com )","title":"Home"},{"location":"#welcome-to-curve-fit","text":"","title":"Welcome to Curve Fit!"},{"location":"#background","text":"CurveFit is a Python package for fitting curves using nonlinear mixed effects. It can be used to do only that if desired. However, due to its current usage for the IHME COVID-19 project , it has modules specifically for evaluating model performance out beyond the range of time observed in the data. Likewise, it has modules for creating uncertainty intervals based on out of sample performance. In our methods documentation we discuss the statistical methods for CurveFit . In our code documentation , we explain the core model code and also the extensions that allow for evaluating model performance and generating uncertainty intervals. NOTE: This documentation is currently under construction and being updated regularly.","title":"Background"},{"location":"#ihme-covid-19-project","text":"For any IHME COVID-19 related inquiries, please contact covid19@healthdata.org . To see the IHME projections visualization, click here . To read the paper, click here . For FAQs, click here . Please note that this code base makes up only one part of the IHME COVID-19 projection process, in particular the COVID-19 deaths forecasting.","title":"IHME COVID-19 Project"},{"location":"#getting-started","text":"To clone the repository and get started, you can either do git clone https://github.com/ihmeuw-msca/CurveFit.git cd CurveFit make install","title":"Getting Started"},{"location":"#a-note-on-cppad_py","text":"One of the dependencies for this package is cppad_py, a python interface for algorithmic differentiation. If you experience issues installing or importing cppad_py after doing make install , please see this page to clone and debug the build for cppad_py with setup.py .","title":"A Note on cppad_py"},{"location":"#maintainers","text":"Aleksandr Aravkin ( saravkin@uw.edu ) Peng Zheng ( zhengp@uw.edu ) Marlena Bannick ( mbannick@uw.edu ) Jize Zhang ( jizez@uw.edu ) Alexey Sholokov ( aksh@uw.edu ) Bradley Bell ( bradbell@seanet.com )","title":"Maintainers"},{"location":"code/","text":"Package Overview For a quick getting started demo, please see getting started . For a demo of using the modeling framework from start to finish, please see model runner . The main modules are listed below, and there is documentation on each of them in the Developer Docs section of the documentation. core : functional forms, objective functions, data management objects, and model parameters models : types of models to fit to the data solvers : types of solvers that are used for the optimization problem of fitting models to data uncertainty : objects for obtaining uncertainty intervals for the forecasts initializer : objects for getting \"smart\" priors based on other datasets utils : utility functions For examples on how to use the code, please see the following Quick Start Using Covariates Using Random Effects Understanding Parametric Functions Initializing Priors Running Models with Predictive Validity","title":"Code"},{"location":"code/#package-overview","text":"For a quick getting started demo, please see getting started . For a demo of using the modeling framework from start to finish, please see model runner . The main modules are listed below, and there is documentation on each of them in the Developer Docs section of the documentation. core : functional forms, objective functions, data management objects, and model parameters models : types of models to fit to the data solvers : types of solvers that are used for the optimization problem of fitting models to data uncertainty : objects for obtaining uncertainty intervals for the forecasts initializer : objects for getting \"smart\" priors based on other datasets utils : utility functions For examples on how to use the code, please see the following Quick Start Using Covariates Using Random Effects Understanding Parametric Functions Initializing Priors Running Models with Predictive Validity","title":"Package Overview"},{"location":"methods/","text":"Overview CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable. COVID-19 functional forms We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Cumulative Distribution Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this. Statistical Model Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p . Constraints Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds. Optimization Procedure The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below. Solver We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task. Derivatives We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms. Uncertainty Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development. Predictive Validity-Based Uncertainty We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below. Model-Based Uncertainty We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Methods"},{"location":"methods/#overview","text":"CurveFit is an extendable nonlinear mixed effects model for fitting curves. The main application in this development is COVID-19 forecasting, so that the curves we consider are variants of logistic models. However the interface allows any user-specified parametrized family. Parametrized curves have several key features that make them useful for forecasting: We can capture key signals from noisy data. Parameters are interpretable, and can be modeled using covariates in a transparent way. Parametric forms allow for more stable inversion approaches, for current and future work. Parametric functions impose rigid assumptions that make forecasting more stable.","title":"Overview"},{"location":"methods/#covid-19-functional-forms","text":"We considered two functional forms so far when modeling the COVID-19 epidemic. Generalized Logistic: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp(-\\alpha(t-\\beta))} Generalized Gaussian Cumulative Distribution Function f(t; \\alpha, \\beta, p) = \\frac{p}{2}\\left(\\Psi(\\alpha(t-\\beta)\\right) = \\frac{p}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha(t-\\beta)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) Each form has comparable fundamental parameters: Level p : Controls the ultimate level. Slope \\alpha : Controls speed of infection. Inflection \\beta : Time at which the rate of change is maximal. We can fit these parameters to data, but this by itself does not account for covariates, and cannot connect different locations together. The next section therefore specifies statistical models that do this.","title":"COVID-19 functional forms"},{"location":"methods/#statistical-model","text":"Statistical assumptions link covariates across locations. Key aspects are the following: Parameters may be influenced by covariates, e.g. those that reflect social distancing Parameters may be modeled in a different space, e.g. p, \\alpha are non-negative Parameters and covariate multipliers may be location-specific, with assumptions placed on their variation. CurveFit specification is tailored to these three requirements. Every parameter in any functional form can be specified through a link function, covariates, fixed, and random effects. The final estimation problem is a nonlinear mixed effects model, with user-specified priors on fixed and random effects. For example, consider the ERF functional form with covariates \\alpha, \\beta, p . Assume we are fitting data in log-cumulative-death-rate space. Input data are: S_j : social distancing covariate value at location j y_j^t : cumulative death rate in location j at time t We specify the statistical model as follows: Measurement model: \\begin{aligned} \\log(y_j^t) &= \\frac{p_j}{2}\\left(1+ \\frac{2}{\\sqrt{\\pi}}\\int_{0}^{\\alpha_j(t-\\beta_j)} \\exp\\left(-\\tau^2\\right)d\\tau\\right) + \\epsilon_{t,j} \\\\ \\epsilon_{t,j} & \\sim N(0, V_t) \\end{aligned} \\beta -model specification: \\begin{aligned} \\beta_j &= \\beta + \\gamma_j S_j + \\epsilon_j^\\beta \\\\ \\gamma_j &\\sim N(\\overline \\gamma, V_\\gamma) \\\\ \\epsilon_j^\\beta &\\sim N(0, V_\\beta) \\end{aligned} \\alpha -model specification: \\begin{aligned} \\alpha_j &= \\exp(\\alpha + u_j^\\alpha) \\\\ u_{\\alpha, j} & \\sim N(0, V_\\alpha) \\end{aligned} p -model specification: \\begin{aligned} p_j & = \\exp(p + u_j^p) \\\\ u_{p,j} & \\sim N(0, V_p) \\end{aligned} In this example, the user specifies prior mean \\overline \\gamma variance parameters V_t, V_\\gamma, V_\\beta, V_\\alpha, V_p . CurveFit estimates: fixed effects \\alpha, \\beta, p random effects \\{\\gamma_j, u_j^\\alpha, u_j^\\beta, u_j^p\\} Exponential link functions are used to model non-negative parameters \\alpha, p .","title":"Statistical Model"},{"location":"methods/#constraints","text":"Simple bound constraints on parameters can be used to make the model more robust. For any fixed or random effect, the user can enter simple bound constraints of the form L \\leq \\theta \\leq U. The parameters returned by CurveFit are guaranteed to satisfy these simple bounds.","title":"Constraints"},{"location":"methods/#optimization-procedure","text":"The optimization problem we obtain from specifying functional forms, priors, and constraints on all parameters is a bound-constrained nonlinear least squares problem. We explain the solver, derivative computation, and initialization procedure below.","title":"Optimization Procedure"},{"location":"methods/#solver","text":"We solve the problem using L-BFGS-B . The L-BFGS-B algorithm uses gradients to build a Hessian approximation, and efficiently uses that approximation and projected gradient method onto the bound constraints to identify parameter spaces over which solutions can be efficiently found, see the paper . It is a standard and robust algorithm that's well suited to the task.","title":"Solver"},{"location":"methods/#derivatives","text":"We do not explicitly compute derivatives of the nonlinear least squares objective induced from the problem specification. Instead, we use the complex step method to do this. The complex step method is a simple example of Automatic Differentiation , that is, it can provide machine precision derivatives at the cost of a function evaluation. This is very useful given the flexibility on functional forms.","title":"Derivatives"},{"location":"methods/#uncertainty","text":"Currently CurveFit uses model-based uncertainty, with out-of-sample approaches under development.","title":"Uncertainty"},{"location":"methods/#predictive-validity-based-uncertainty","text":"We have a tool that evaluates predictive validity out of sample for the model forecasts. It iteratively holds out data points starting with only one data point used for fitting and adding them back in one by one, comparing the predictions with the observed data. The standard deviation observed for these residuals -- along the dimensions of how much data the model sees and how far out the model needs to predict into the future -- are then used to simulate draws (random realizations of the mean function) that can be used to construct uncertainty intervals. This approach is orthogonal to model-based uncertainty described below.","title":"Predictive Validity-Based Uncertainty"},{"location":"methods/#model-based-uncertainty","text":"We partition model-based uncertainty into estimates coming from fixed and random components. Fixed effects capture the variation of the mean effects, and random effects uncertainty captures the variation across locations. Fixed Effects For any estimator obtained by solving a nonlinear least squares problem, we can use the Fisher information matrix to get an asymptotic approximation to the uncertainty. Let \\hat{ \\theta} = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta^T W^{-1} \\theta + \\frac{1}{2 \\sigma^2} \\|\\Sigma^{-1/2} (y - f( \\theta; X))\\|^2 where W is any prior variance and \\Sigma is the variance of observations. Then our approximation for the variance matrix of the estimate is given by V(\\hat \\theta) = \\mathcal{I}(\\theta)^{-1} = \\left(J_{\\hat \\theta}^T \\Sigma^{-1} J_{\\hat \\theta} + W^{-1} \\right)^{-1} where J_{\\hat{ \\theta}} := \\nabla_{ \\theta} f( \\theta; X) is the Jacobian matrix evaluated at \\theta = \\hat \\theta . The Jacobian is also computed using the complex step method. Random effects To obtain the variance of the random effects, we derive an empirical variance matrix across locations. Given a set of zero mean random effect estimates \\{v_j\\} , with each v_j a vector of k of random effect types, we get an empirical matrix V_0 \\in \\mathbb{R}^{k\\times k} by V_0 = \\frac{1}{n}\\sum_{j=1}^N v_j v_j^T To obtain posterior uncertainty for each specific location, we use the empirical V_0 as a prior, and any data at the location as the measurement model, and re-fit the location: \\hat{ \\theta}_i = \\arg\\min_{ \\theta} := \\frac{1}{2}\\theta_i^T V_0^{-1}\\theta_i + \\frac{1}{2 \\sigma^2} \\| \\Sigma_i^{-1/2}(y_i - f_i( \\theta_i; X_i))\\|^2 Within each location, this is analogous to the fixed effects analysis. The location-specific uncertainty is then estimated from the same Fisher information analysis: V_i({\\hat \\theta}) = ( J_i^T \\Sigma_i ^{-1} J_i + V_0^{-1})^{-1}.","title":"Model-Based Uncertainty"},{"location":"updates/","text":"Release Notes 1.0.0: Large refactor of code structure 0.0.0 ()","title":"Release Notes"},{"location":"updates/#release-notes","text":"1.0.0: Large refactor of code structure 0.0.0 ()","title":"Release Notes"},{"location":"extract_md/BetaPrior/","text":"curvefit.initializer.initializer.BetaPrior Gets information about a beta prior for a model See PriorInitializerComponent for a description of a prior initializer. This prior initializer component uses one joint model fit that has beta as a parameter. Typically this prior is used only on a subset of groups that have more information about what beta should be. This prior initializer component uses the fixed effect mean as the new prior mean and the standard deviation of the random effects as the prior standard deviation for beta. NOTE: Requires a 'beta' parameter in a ParameterSet. Attributes See attributes for IndividualPriorInitializerComponent . Methods See methods for IndividualPriorInitializerComponent .","title":"BetaPrior"},{"location":"extract_md/BetaPrior/#curvefitinitializerinitializerbetaprior","text":"","title":"curvefit.initializer.initializer.BetaPrior"},{"location":"extract_md/BetaPrior/#gets-information-about-a-beta-prior-for-a-model","text":"See PriorInitializerComponent for a description of a prior initializer. This prior initializer component uses one joint model fit that has beta as a parameter. Typically this prior is used only on a subset of groups that have more information about what beta should be. This prior initializer component uses the fixed effect mean as the new prior mean and the standard deviation of the random effects as the prior standard deviation for beta. NOTE: Requires a 'beta' parameter in a ParameterSet.","title":"Gets information about a beta prior for a model"},{"location":"extract_md/BetaPrior/#attributes","text":"See attributes for IndividualPriorInitializerComponent .","title":"Attributes"},{"location":"extract_md/BetaPrior/#methods","text":"See methods for IndividualPriorInitializerComponent .","title":"Methods"},{"location":"extract_md/CompositeSolver/","text":"curvefit.solvers.solvers.CompositeSolver Composite Solver General base class for a solver with multiple elements. Used to create composites of solvers. Arguments solver (curvefit.solvers.solvers.Solver) : a base solver to build the composite off of See Solver for more arguments and methods.","title":"CompositeSolver"},{"location":"extract_md/CompositeSolver/#curvefitsolverssolverscompositesolver","text":"","title":"curvefit.solvers.solvers.CompositeSolver"},{"location":"extract_md/CompositeSolver/#composite-solver","text":"General base class for a solver with multiple elements. Used to create composites of solvers.","title":"Composite Solver"},{"location":"extract_md/CompositeSolver/#arguments","text":"solver (curvefit.solvers.solvers.Solver) : a base solver to build the composite off of See Solver for more arguments and methods.","title":"Arguments"},{"location":"extract_md/CoreModel/","text":"curvefit.core.core_model.Model Base class for a curvefit model Add description here. Arguments param_set (curvefit.core.parameter.ParameterSet) curve_fun (Callable) : function from curvefit.core.functions for the parametric function to fit loss_fun (Callable) : function from curvefit.core.functions for the loss function Attributes self.data_inputs (curvefit.models.base.DataInputs) : data inputs that have been converted during data fitting -- helper for the objective function Methods objective Returns a function that can be called in a Solver that is the objective function given the current variables and data. x (np.array) : an array of variable values that can be converted to parameters, these will be the parameters that the objective function is evaluated at data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object get_params Wrapper for effects2params to convert the values of x (the variables) into parameters for the model. x (np.array) : an array of variable values that can be converted to parameters predict Create predictions given some variable values x and at some times t . Can optionally pass a different functional form as long as it is in the same family (e.g. Gaussian). x (np.array) : an array of variable values that can be converted to parameters t (np.array) : times to evaluate the function predict_fun (Callable) : function from curvefit.core.functions is_multi_groups (bool) : whether or not the model was fit on data for multiple groups convert_inputs Convert a data frame and specifications into inputs for the objective function of the model. data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object","title":"CoreModel"},{"location":"extract_md/CoreModel/#curvefitcorecore_modelmodel","text":"","title":"curvefit.core.core_model.Model"},{"location":"extract_md/CoreModel/#base-class-for-a-curvefit-model","text":"Add description here.","title":"Base class for a curvefit model"},{"location":"extract_md/CoreModel/#arguments","text":"param_set (curvefit.core.parameter.ParameterSet) curve_fun (Callable) : function from curvefit.core.functions for the parametric function to fit loss_fun (Callable) : function from curvefit.core.functions for the loss function","title":"Arguments"},{"location":"extract_md/CoreModel/#attributes","text":"self.data_inputs (curvefit.models.base.DataInputs) : data inputs that have been converted during data fitting -- helper for the objective function","title":"Attributes"},{"location":"extract_md/CoreModel/#methods","text":"","title":"Methods"},{"location":"extract_md/CoreModel/#objective","text":"Returns a function that can be called in a Solver that is the objective function given the current variables and data. x (np.array) : an array of variable values that can be converted to parameters, these will be the parameters that the objective function is evaluated at data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object","title":"objective"},{"location":"extract_md/CoreModel/#get_params","text":"Wrapper for effects2params to convert the values of x (the variables) into parameters for the model. x (np.array) : an array of variable values that can be converted to parameters","title":"get_params"},{"location":"extract_md/CoreModel/#predict","text":"Create predictions given some variable values x and at some times t . Can optionally pass a different functional form as long as it is in the same family (e.g. Gaussian). x (np.array) : an array of variable values that can be converted to parameters t (np.array) : times to evaluate the function predict_fun (Callable) : function from curvefit.core.functions is_multi_groups (bool) : whether or not the model was fit on data for multiple groups","title":"predict"},{"location":"extract_md/CoreModel/#convert_inputs","text":"Convert a data frame and specifications into inputs for the objective function of the model. data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object","title":"convert_inputs"},{"location":"extract_md/Data/","text":"curvefit.core.data.Data Manages all data to be used for curve fitting The data class contains all information about the complete dataset, list of groups, column names. Creates observation standard error based on obs_se_func . All sorting will happen immediately in the Data class init and we won\u2019t have to sort again. NOTE: will prioritize obs_se_func over col_obs_se . Syntax d = Data ( df , col_t , col_obs , obs_space , col_covs , col_group ) Arguments df (pandas.DataFrame) : all data col_t (str) : column with time col_obs (str) : column with observation values obs_space (callable) : function from functions module that specifies what space the col_obs column represents (e.g. ln_gaussian_cdf) col_covs (List[str]) : covariate columns col_group (str) : group column col_obs_se (optional, str) : column with observation standard error specified. obs_se_func (optional, callable) : observation standard error function to create from time Attributes self.groups (List[str]) : list of groups, sorted in order self.num_obs (int) : dictionary of numbers of observations per group (key) self.times (Dict[str: np.array])) : dictionary of times per group (key) Methods _get_df Returns a copy of the data frame or a pointer to the data frame, and optionally data specs. If you plan on modifying the data frame, use copy=True . If a group is passed, then a group-specific data frame will be passed. group (optional, str) : optional group name copy (bool) : return a copy or not _get_translated_observations Returns the observations for a group in a particular space from curvefit.core.functions group (str) : which group to return observations for space (callable) : which space to translate observations to","title":"Data"},{"location":"extract_md/Data/#curvefitcoredatadata","text":"","title":"curvefit.core.data.Data"},{"location":"extract_md/Data/#manages-all-data-to-be-used-for-curve-fitting","text":"The data class contains all information about the complete dataset, list of groups, column names. Creates observation standard error based on obs_se_func . All sorting will happen immediately in the Data class init and we won\u2019t have to sort again. NOTE: will prioritize obs_se_func over col_obs_se .","title":"Manages all data to be used for curve fitting"},{"location":"extract_md/Data/#syntax","text":"d = Data ( df , col_t , col_obs , obs_space , col_covs , col_group )","title":"Syntax"},{"location":"extract_md/Data/#arguments","text":"df (pandas.DataFrame) : all data col_t (str) : column with time col_obs (str) : column with observation values obs_space (callable) : function from functions module that specifies what space the col_obs column represents (e.g. ln_gaussian_cdf) col_covs (List[str]) : covariate columns col_group (str) : group column col_obs_se (optional, str) : column with observation standard error specified. obs_se_func (optional, callable) : observation standard error function to create from time","title":"Arguments"},{"location":"extract_md/Data/#attributes","text":"self.groups (List[str]) : list of groups, sorted in order self.num_obs (int) : dictionary of numbers of observations per group (key) self.times (Dict[str: np.array])) : dictionary of times per group (key)","title":"Attributes"},{"location":"extract_md/Data/#methods","text":"","title":"Methods"},{"location":"extract_md/Data/#_get_df","text":"Returns a copy of the data frame or a pointer to the data frame, and optionally data specs. If you plan on modifying the data frame, use copy=True . If a group is passed, then a group-specific data frame will be passed. group (optional, str) : optional group name copy (bool) : return a copy or not","title":"_get_df"},{"location":"extract_md/Data/#_get_translated_observations","text":"Returns the observations for a group in a particular space from curvefit.core.functions group (str) : which group to return observations for space (callable) : which space to translate observations to","title":"_get_translated_observations"},{"location":"extract_md/DataInputs/","text":"curvefit.models.base.DataInputs Provides the required data inputs for a curvefit.core.core_model.Model The DataInputs class holds all of the inputs that are needed for fitting a core model. It is only used in the Model.convert_inputs() method ( see here . The purpose is to extract only the required elements of a Data class that are needed for model fitting in order to reduce the memory usage, but also keep key information for model debugging. Arguments t (np.ndarray) : the time variable (or independent variable) in the curve fitting obs (np.ndarray) : the observation variable (or dependent variable) in the curve fitting obs_se (np.ndarray) : the observation standard error to attach to the observations covariates_matrices (List[np.ndarray]) : list of covariate matrices for each parameter (in many cases these covariate matrices will just be one column of ones) group_sizes (List[int]) : size of the groups num_groups (int) : number of groups link_fun (List[Callable]) : list of link functions for the parameters var_link_fun (List[Callable]) : list of variable link functions for the variables x_init (np.ndarray) : initial values for variables bounds (np.ndarray) : bounds for variables fe_gprior (np.ndarray) : array of fixed effects Gaussian priors for the variables re_gprior (np.ndarray) : array of random effects Gaussian priors for the variables param_gprior_info (Tuple[Callable, Tuple[List[float], List[float]]]) : tuple of information about the parameter functional Gaussian priors; first element is a composite function of all of the parameter functional priors; second element is another tuple and the first element is a list of means, the second element is a list of standard deviations re_zero_sum_std: (np.ndarray) : is a vector with length equal to the number of fixed effects. It j-th component is the standard deviation of the zero sum of the random effects corresponding to the j-th fixed effect.","title":"DataInputs"},{"location":"extract_md/DataInputs/#curvefitmodelsbasedatainputs","text":"","title":"curvefit.models.base.DataInputs"},{"location":"extract_md/DataInputs/#provides-the-required-data-inputs-for-a-curvefitcorecore_modelmodel","text":"The DataInputs class holds all of the inputs that are needed for fitting a core model. It is only used in the Model.convert_inputs() method ( see here . The purpose is to extract only the required elements of a Data class that are needed for model fitting in order to reduce the memory usage, but also keep key information for model debugging.","title":"Provides the required data inputs for a curvefit.core.core_model.Model"},{"location":"extract_md/DataInputs/#arguments","text":"t (np.ndarray) : the time variable (or independent variable) in the curve fitting obs (np.ndarray) : the observation variable (or dependent variable) in the curve fitting obs_se (np.ndarray) : the observation standard error to attach to the observations covariates_matrices (List[np.ndarray]) : list of covariate matrices for each parameter (in many cases these covariate matrices will just be one column of ones) group_sizes (List[int]) : size of the groups num_groups (int) : number of groups link_fun (List[Callable]) : list of link functions for the parameters var_link_fun (List[Callable]) : list of variable link functions for the variables x_init (np.ndarray) : initial values for variables bounds (np.ndarray) : bounds for variables fe_gprior (np.ndarray) : array of fixed effects Gaussian priors for the variables re_gprior (np.ndarray) : array of random effects Gaussian priors for the variables param_gprior_info (Tuple[Callable, Tuple[List[float], List[float]]]) : tuple of information about the parameter functional Gaussian priors; first element is a composite function of all of the parameter functional priors; second element is another tuple and the first element is a list of means, the second element is a list of standard deviations re_zero_sum_std: (np.ndarray) : is a vector with length equal to the number of fixed effects. It j-th component is the standard deviation of the zero sum of the random effects corresponding to the j-th fixed effect.","title":"Arguments"},{"location":"extract_md/DataSpecs/","text":"curvefit.core.data.DataSpecs Data specifications based on a Data object Arguments See arguments to curvefit.core.data.Data .","title":"DataSpecs"},{"location":"extract_md/DataSpecs/#curvefitcoredatadataspecs","text":"","title":"curvefit.core.data.DataSpecs"},{"location":"extract_md/DataSpecs/#data-specifications-based-on-a-data-object","text":"","title":"Data specifications based on a Data object"},{"location":"extract_md/DataSpecs/#arguments","text":"See arguments to curvefit.core.data.Data .","title":"Arguments"},{"location":"extract_md/Draws/","text":"curvefit.uncertainty.draws.Draws A class for generating draws: predictions plus random residuals according to provided ResidualModel This class simulates possible trajectories by adjusting out-of-sample part of the prediction with expected residuals from ResidualModel. The user-controlled parameters are passed via init; the parameters which need to be consistent to other parts of the pipeline (like model , solver , evaluation_space , etc) are passed via create_draws(...). Arguments num_draws (int) : the number of draws to take prediction_times (np.array) : which times to produce final predictions (draws) at Methods create_draws Fills in the `draws` dictionary by generating `num_groups` sample trajectories per location - `data (curvefit.core.data.Data)` : data - `model_prototype (curvefit.models.base.Model)` : a model object - `solver_prototype (curvefit.solvers.solver.Solver)` : a solver used to fit the model - `residual_model (curvefit.uncertainty.residual_model._ResidualModel)' a residual model object, should be fitted before being passed here. - ` evaluation_space ( callable ) `: which space to generate draws in. It should be the same as used in ` PredictiveValidity ` instance which generated the residual matrix passed to the ` residual_model ` - ` theta ( float ) ` : between 0 and 1 , how much scaling of the residuals to do relative to the prediction mean get_draws Returns generated draws - ` group ( str ) ` : Group for which the draws are requested . Defaults to None , in which case returns a dictionary of all draws indexed by groups . get_draws_summary Returns mean and ( lower , higher ) quantiles of draws - ` group ( str ) ` : Group for which draws summary is requested . Defaults to None , in which case returns a dictionary of statistics for all draws indexed by groups . - ` quantiles ( float ) ` : which quantiles to generate . Should be between 0 and 0 . 5 . For instance , if quantiles = 0 . 05 is passed then it returns 5 'th and 95' th percentiles . Usage In ModelRunner.run() : draws = self . draws . create_draws ( data = self . data , model_prototype = self . model , solver_prototype = self . solver , residual_model = self . residual_model , evaluation_space = self . predictive_validity . evaluation_space , theta = self . predictive_validity . theta ) . get_draws ()","title":"Draws"},{"location":"extract_md/Draws/#curvefituncertaintydrawsdraws","text":"","title":"curvefit.uncertainty.draws.Draws"},{"location":"extract_md/Draws/#a-class-for-generating-draws-predictions-plus-random-residuals-according-to-provided-residualmodel","text":"This class simulates possible trajectories by adjusting out-of-sample part of the prediction with expected residuals from ResidualModel. The user-controlled parameters are passed via init; the parameters which need to be consistent to other parts of the pipeline (like model , solver , evaluation_space , etc) are passed via create_draws(...).","title":"A class for generating draws: predictions plus random residuals according to provided ResidualModel"},{"location":"extract_md/Draws/#arguments","text":"num_draws (int) : the number of draws to take prediction_times (np.array) : which times to produce final predictions (draws) at","title":"Arguments"},{"location":"extract_md/Draws/#methods","text":"","title":"Methods"},{"location":"extract_md/Draws/#create_draws","text":"Fills in the `draws` dictionary by generating `num_groups` sample trajectories per location - `data (curvefit.core.data.Data)` : data - `model_prototype (curvefit.models.base.Model)` : a model object - `solver_prototype (curvefit.solvers.solver.Solver)` : a solver used to fit the model - `residual_model (curvefit.uncertainty.residual_model._ResidualModel)' a residual model object, should be fitted before being passed here. - ` evaluation_space ( callable ) `: which space to generate draws in. It should be the same as used in ` PredictiveValidity ` instance which generated the residual matrix passed to the ` residual_model ` - ` theta ( float ) ` : between 0 and 1 , how much scaling of the residuals to do relative to the prediction mean","title":"create_draws"},{"location":"extract_md/Draws/#get_draws","text":"Returns generated draws - ` group ( str ) ` : Group for which the draws are requested . Defaults to None , in which case returns a dictionary of all draws indexed by groups .","title":"get_draws"},{"location":"extract_md/Draws/#get_draws_summary","text":"Returns mean and ( lower , higher ) quantiles of draws - ` group ( str ) ` : Group for which draws summary is requested . Defaults to None , in which case returns a dictionary of statistics for all draws indexed by groups . - ` quantiles ( float ) ` : which quantiles to generate . Should be between 0 and 0 . 5 . For instance , if quantiles = 0 . 05 is passed then it returns 5 'th and 95' th percentiles .","title":"get_draws_summary"},{"location":"extract_md/Draws/#usage","text":"In ModelRunner.run() : draws = self . draws . create_draws ( data = self . data , model_prototype = self . model , solver_prototype = self . solver , residual_model = self . residual_model , evaluation_space = self . predictive_validity . evaluation_space , theta = self . predictive_validity . theta ) . get_draws ()","title":"Usage"},{"location":"extract_md/GaussianMixtures/","text":"curvefit.models.gaussian_mixtures.GaussianMixtures Fit a linear combination of some Gaussian distributions The GaussianMixtures model allows you to fit weights for a linear combination of Gaussian distributions using least squares. It is typically used on top of a CoreModel . Arguments stride (int) : the distance between the Gaussian atoms; how far to space them out over the independent variable size (int) : number of Gaussian atoms to include, replicated over different locations determined by stride params (np.array) : parameters to build the Gaussian distributions; must be parameters from a Gaussian family function in curvefit.core.functions Methods compute_design_matrix Calculates the design matrix from the Gaussian atoms that will be used in order to get their optimal linear combination. t (np.array) : how far in time should the predictions that will go into the design matrix extend See additional methods in CoreModel .","title":"GaussianMixtures"},{"location":"extract_md/GaussianMixtures/#curvefitmodelsgaussian_mixturesgaussianmixtures","text":"","title":"curvefit.models.gaussian_mixtures.GaussianMixtures"},{"location":"extract_md/GaussianMixtures/#fit-a-linear-combination-of-some-gaussian-distributions","text":"The GaussianMixtures model allows you to fit weights for a linear combination of Gaussian distributions using least squares. It is typically used on top of a CoreModel .","title":"Fit a linear combination of some Gaussian distributions"},{"location":"extract_md/GaussianMixtures/#arguments","text":"stride (int) : the distance between the Gaussian atoms; how far to space them out over the independent variable size (int) : number of Gaussian atoms to include, replicated over different locations determined by stride params (np.array) : parameters to build the Gaussian distributions; must be parameters from a Gaussian family function in curvefit.core.functions","title":"Arguments"},{"location":"extract_md/GaussianMixtures/#methods","text":"","title":"Methods"},{"location":"extract_md/GaussianMixtures/#compute_design_matrix","text":"Calculates the design matrix from the Gaussian atoms that will be used in order to get their optimal linear combination. t (np.array) : how far in time should the predictions that will go into the design matrix extend See additional methods in CoreModel .","title":"compute_design_matrix"},{"location":"extract_md/GaussianMixturesIntegration/","text":"curvefit.solvers.solvers.GaussianMixturesIntegration GaussianMixturesIntegration Solver The solver that is used to find the linear combination of Gaussian atoms built on top of one core model instance. Arguments gm_model (curvefit.models.gaussian_mixtures.GaussianMixtures) : some a model instance of a gaussian mixture model See CompositeSolver for more arguments and methods.","title":"GaussianMixturesIntegration"},{"location":"extract_md/GaussianMixturesIntegration/#curvefitsolverssolversgaussianmixturesintegration","text":"","title":"curvefit.solvers.solvers.GaussianMixturesIntegration"},{"location":"extract_md/GaussianMixturesIntegration/#gaussianmixturesintegration-solver","text":"The solver that is used to find the linear combination of Gaussian atoms built on top of one core model instance.","title":"GaussianMixturesIntegration Solver"},{"location":"extract_md/GaussianMixturesIntegration/#arguments","text":"gm_model (curvefit.models.gaussian_mixtures.GaussianMixtures) : some a model instance of a gaussian mixture model See CompositeSolver for more arguments and methods.","title":"Arguments"},{"location":"extract_md/IndividualPriorInitializerComponent/","text":"curvefit.initializer.initializer.IndividualPriorInitializerComponent An individual prior initializer component for an initializer for a model See PriorInitializerComponent . Attributes self.component_type (str) : \"individual\", which is the type of component Methods See methods for PriorInitializerComponent .","title":"IndividualPriorInitializerComponent"},{"location":"extract_md/IndividualPriorInitializerComponent/#curvefitinitializerinitializerindividualpriorinitializercomponent","text":"","title":"curvefit.initializer.initializer.IndividualPriorInitializerComponent"},{"location":"extract_md/IndividualPriorInitializerComponent/#an-individual-prior-initializer-component-for-an-initializer-for-a-model","text":"See PriorInitializerComponent .","title":"An individual prior initializer component for an initializer for a model"},{"location":"extract_md/IndividualPriorInitializerComponent/#attributes","text":"self.component_type (str) : \"individual\", which is the type of component","title":"Attributes"},{"location":"extract_md/IndividualPriorInitializerComponent/#methods","text":"See methods for PriorInitializerComponent .","title":"Methods"},{"location":"extract_md/JointPriorInitializerComponent/","text":"curvefit.initializer.initializer.JointPriorInitializerComponent A joint prior initializer component for an initializer for a model See PriorInitializerComponent . Attributes self.component_type (str) : \"joint\", which is the type of component Methods See methods for PriorInitializerComponent .","title":"JointPriorInitializerComponent"},{"location":"extract_md/JointPriorInitializerComponent/#curvefitinitializerinitializerjointpriorinitializercomponent","text":"","title":"curvefit.initializer.initializer.JointPriorInitializerComponent"},{"location":"extract_md/JointPriorInitializerComponent/#a-joint-prior-initializer-component-for-an-initializer-for-a-model","text":"See PriorInitializerComponent .","title":"A joint prior initializer component for an initializer for a model"},{"location":"extract_md/JointPriorInitializerComponent/#attributes","text":"self.component_type (str) : \"joint\", which is the type of component","title":"Attributes"},{"location":"extract_md/JointPriorInitializerComponent/#methods","text":"See methods for PriorInitializerComponent .","title":"Methods"},{"location":"extract_md/LnAlphaBetaPrior/","text":"curvefit.initializer.initializer.LnAlphaBetaPrior Gets information about a ln alpha-beta prior for model See PriorInitializerComponent for a description of a prior initializer. This prior initializer component uses group-specific individual model fits with alpha and beta parameters to get information about what the mean and standard deviation for a functional prior on log(alpha * beta) should be. It uses the empirical mean and standard deviation of log(alpha * beta), typically fit on only a subset of groups that have more data and information about what alpha and beta should be. NOTE: Requires 'alpha' and 'beta' parameters in a ParameterSet along with a functional prior called 'ln-alpha-beta'. Attributes See attributes for IndividualPriorInitializerComponent . Methods See methods for IndividualPriorInitializerComponent .","title":"LnAlphaBetaPrior"},{"location":"extract_md/LnAlphaBetaPrior/#curvefitinitializerinitializerlnalphabetaprior","text":"","title":"curvefit.initializer.initializer.LnAlphaBetaPrior"},{"location":"extract_md/LnAlphaBetaPrior/#gets-information-about-a-ln-alpha-beta-prior-for-model","text":"See PriorInitializerComponent for a description of a prior initializer. This prior initializer component uses group-specific individual model fits with alpha and beta parameters to get information about what the mean and standard deviation for a functional prior on log(alpha * beta) should be. It uses the empirical mean and standard deviation of log(alpha * beta), typically fit on only a subset of groups that have more data and information about what alpha and beta should be. NOTE: Requires 'alpha' and 'beta' parameters in a ParameterSet along with a functional prior called 'ln-alpha-beta'.","title":"Gets information about a ln alpha-beta prior for model"},{"location":"extract_md/LnAlphaBetaPrior/#attributes","text":"See attributes for IndividualPriorInitializerComponent .","title":"Attributes"},{"location":"extract_md/LnAlphaBetaPrior/#methods","text":"See methods for IndividualPriorInitializerComponent .","title":"Methods"},{"location":"extract_md/Model/","text":"curvefit.models.base.Model Base model The base model; must be subclassed. See CoreModel . Methods get_data Returns the DataInputs object that was used to fit the model; or None. erase_data Get rid of the current data in the model. detach_data Get rid of the current data in the model but return them from the function. gradient Returns the gradient function using a complex step based on some current variable values x and DataInputs . x (np.array) : an array of variable values that can be converted to parameters data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object","title":"Model"},{"location":"extract_md/Model/#curvefitmodelsbasemodel","text":"","title":"curvefit.models.base.Model"},{"location":"extract_md/Model/#base-model","text":"The base model; must be subclassed. See CoreModel .","title":"Base model"},{"location":"extract_md/Model/#methods","text":"","title":"Methods"},{"location":"extract_md/Model/#get_data","text":"Returns the DataInputs object that was used to fit the model; or None.","title":"get_data"},{"location":"extract_md/Model/#erase_data","text":"Get rid of the current data in the model.","title":"erase_data"},{"location":"extract_md/Model/#detach_data","text":"Get rid of the current data in the model but return them from the function.","title":"detach_data"},{"location":"extract_md/Model/#gradient","text":"Returns the gradient function using a complex step based on some current variable values x and DataInputs . x (np.array) : an array of variable values that can be converted to parameters data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object","title":"gradient"},{"location":"extract_md/MultipleInitialization/","text":"curvefit.solvers.solvers.MultipleInitialization MultipleInitialization Solver Uses a sampling function to sample initial values around the initial values specified in the model instance, and picks the initial values which attain lowest objective function value. Arguments sample_fun (Callable) : some function to use to sample initial points around the initial points specified in the model instance See CompositeSolver for more arguments and methods.","title":"MultipleInitialization"},{"location":"extract_md/MultipleInitialization/#curvefitsolverssolversmultipleinitialization","text":"","title":"curvefit.solvers.solvers.MultipleInitialization"},{"location":"extract_md/MultipleInitialization/#multipleinitialization-solver","text":"Uses a sampling function to sample initial values around the initial values specified in the model instance, and picks the initial values which attain lowest objective function value.","title":"MultipleInitialization Solver"},{"location":"extract_md/MultipleInitialization/#arguments","text":"sample_fun (Callable) : some function to use to sample initial points around the initial points specified in the model instance See CompositeSolver for more arguments and methods.","title":"Arguments"},{"location":"extract_md/Parameter/","text":"curvefit.core.parameter.Parameter A parameter for the functional form of the curve A Parameter is a parameter of the functional form for the curve. For example, if the parametric curve you want to fit has three parameters then you need 3 Parameter objects. A Parameter is made up of one or more Variable objects, which represent the fixed and random effects. Parameters may have link functions that transform the parameter into some other space to enforce, for example, positivity of the parameter (e.g. the parameter representing the scale of a Gaussian distribution can't be negative). Arguments param_name (str) : name of parameter, e.g. 'alpha' link_fun (Callable) : the link function for the parameter variables (List[curvefit.core.parameter.Variable]) : a list of Variable instances Attributes All attributes from the Variable s in the list in the variables argument are carried over to Parameter but they are put into a list. For example, the fe_init attribute for Parameter is a list of fe_init attributes for each Variable in the order that they were passed in variables list. Additional attributes that are not lists of the individual Variable attributes are listed below. self.num_fe (int) : total number of effects for the parameter (number of variables) Usage from curvefit.core.parameter import Parameter , Variable var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. ) param = Parameter ( param_name = 'alpha' , link_fun = lambda x : x , variables = [ var ])","title":"Parameter"},{"location":"extract_md/Parameter/#curvefitcoreparameterparameter","text":"","title":"curvefit.core.parameter.Parameter"},{"location":"extract_md/Parameter/#a-parameter-for-the-functional-form-of-the-curve","text":"A Parameter is a parameter of the functional form for the curve. For example, if the parametric curve you want to fit has three parameters then you need 3 Parameter objects. A Parameter is made up of one or more Variable objects, which represent the fixed and random effects. Parameters may have link functions that transform the parameter into some other space to enforce, for example, positivity of the parameter (e.g. the parameter representing the scale of a Gaussian distribution can't be negative).","title":"A parameter for the functional form of the curve"},{"location":"extract_md/Parameter/#arguments","text":"param_name (str) : name of parameter, e.g. 'alpha' link_fun (Callable) : the link function for the parameter variables (List[curvefit.core.parameter.Variable]) : a list of Variable instances","title":"Arguments"},{"location":"extract_md/Parameter/#attributes","text":"All attributes from the Variable s in the list in the variables argument are carried over to Parameter but they are put into a list. For example, the fe_init attribute for Parameter is a list of fe_init attributes for each Variable in the order that they were passed in variables list. Additional attributes that are not lists of the individual Variable attributes are listed below. self.num_fe (int) : total number of effects for the parameter (number of variables)","title":"Attributes"},{"location":"extract_md/Parameter/#usage","text":"from curvefit.core.parameter import Parameter , Variable var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. ) param = Parameter ( param_name = 'alpha' , link_fun = lambda x : x , variables = [ var ])","title":"Usage"},{"location":"extract_md/ParameterSet/","text":"curvefit.core.parameter.ParameterSet A set of parameters that together specify the functional form of a curve A ParameterSet is a set of parameters that define the functional form for the curve. For example, if the parametric curve you want to fit has three parameters then you need 1 ParameterSet objects that consists of 3 Parameter objects. A ParameterSet is made up of one or more Parameter objects, which are each made up of one or more Variable objects. Please refer to their documentation for more details on those objects. A ParameterSet can also encode functional priors -- priors for functions of the parameter list that is passed into a ParameterSet . Arguments parameters (List[curvefit.core.parameter.Parameter]) : a list of Parameter instances parameter_functions (List[Tuple[Callable, List[float]]] : a list of tuples which each contain (0) functions to apply to the parameters list and (1) a prior for the parameter function (mean and standard deviation -- see Variable for more details about priors) Attributes All attributes from the Parameter s in the list in the parameters argument are carried over to ParameterSet but they are put into a list. For example, the fe_init attribute for ParameterSet is a list of the fe_init attributes for each Parameter in the order that they were passed in parameters list (which are lists of fe_inits for each Variable within a Parameter (see here for more). Additional attributes that are not lists of the individual Parameter attributes are listed below. self.num_fe (int) : total number of effects for the parameter set (number of variables) Methods delete_random_effects Returns a copy of itself but with random effects bounds set to 0. This means that the parameter set will not have any random effects in the model. Useful for when the same parameter set will be used to fit jointly to many groups before being fit to individual groups. Usage from curvefit.core.parameter import Parameter , Variable , ParameterSet var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. ) param = Parameter ( param_name = 'alpha' , link_fun = lambda x : x , variables = [ var ]) param_function = ParameterFunction ( param_function_name = 'alpha_squared' , param_function = lambda params : params [ 0 ] ** 2 , param_function_fe_gprior = [ 0. , np . inf ] ) param_set = ParameterSet ( parameters = [ param ], parameter_functions = [ param_function ] )","title":"ParameterSet"},{"location":"extract_md/ParameterSet/#curvefitcoreparameterparameterset","text":"","title":"curvefit.core.parameter.ParameterSet"},{"location":"extract_md/ParameterSet/#a-set-of-parameters-that-together-specify-the-functional-form-of-a-curve","text":"A ParameterSet is a set of parameters that define the functional form for the curve. For example, if the parametric curve you want to fit has three parameters then you need 1 ParameterSet objects that consists of 3 Parameter objects. A ParameterSet is made up of one or more Parameter objects, which are each made up of one or more Variable objects. Please refer to their documentation for more details on those objects. A ParameterSet can also encode functional priors -- priors for functions of the parameter list that is passed into a ParameterSet .","title":"A set of parameters that together specify the functional form of a curve"},{"location":"extract_md/ParameterSet/#arguments","text":"parameters (List[curvefit.core.parameter.Parameter]) : a list of Parameter instances parameter_functions (List[Tuple[Callable, List[float]]] : a list of tuples which each contain (0) functions to apply to the parameters list and (1) a prior for the parameter function (mean and standard deviation -- see Variable for more details about priors)","title":"Arguments"},{"location":"extract_md/ParameterSet/#attributes","text":"All attributes from the Parameter s in the list in the parameters argument are carried over to ParameterSet but they are put into a list. For example, the fe_init attribute for ParameterSet is a list of the fe_init attributes for each Parameter in the order that they were passed in parameters list (which are lists of fe_inits for each Variable within a Parameter (see here for more). Additional attributes that are not lists of the individual Parameter attributes are listed below. self.num_fe (int) : total number of effects for the parameter set (number of variables)","title":"Attributes"},{"location":"extract_md/ParameterSet/#methods","text":"","title":"Methods"},{"location":"extract_md/ParameterSet/#delete_random_effects","text":"Returns a copy of itself but with random effects bounds set to 0. This means that the parameter set will not have any random effects in the model. Useful for when the same parameter set will be used to fit jointly to many groups before being fit to individual groups.","title":"delete_random_effects"},{"location":"extract_md/ParameterSet/#usage","text":"from curvefit.core.parameter import Parameter , Variable , ParameterSet var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. ) param = Parameter ( param_name = 'alpha' , link_fun = lambda x : x , variables = [ var ]) param_function = ParameterFunction ( param_function_name = 'alpha_squared' , param_function = lambda params : params [ 0 ] ** 2 , param_function_fe_gprior = [ 0. , np . inf ] ) param_set = ParameterSet ( parameters = [ param ], parameter_functions = [ param_function ] )","title":"Usage"},{"location":"extract_md/PredictiveValidity/","text":"curvefit.uncertainty.predictive_validity Out of sample predictive validity Runs out of sample predictive validity for all groups in the data with the specified model and solver. Uses a Residuals object for each group to store predictions and compute out of sample residuals at each time point. Arguments evaluation_space (Callable) : the space in which to do predictive validity, can be different than the space that you're fitting the model in debug_mode (bool) : whether or not to store all of the model data and results used for every single group-specific model (saves a lot of memory when debug_mode = False ) Attributes theta (int) : the amount of scaling for the residuals relative to the predictions. If the evaluation space is in log space, then theta is set to 0. so that they are absolute residuals. If the evaluation space is in linear space, then theta is set to 1. so that they are relative residuals to the prediction magnitude. `group_residuals (Dict[str: curvefit.uncertainty.residuals.Residual]): residual objects for each group in the dataset `group_records (Dict[str: List[curvefit.solvers.solver.Solver]]): a list of solvers at each time point in the predictive validity analysis for a particular group Methods _make_group_info Creates a ResidualInfo object based on the data passed in and for a particular group. data (curvefit.core.data.DataSpecs) : data specifications group_name (str) : the name of the group _run_group_pv Runs the predictive validity analysis for only one group using the data , model and solver from the run_predictive_validity() function. run_predictive_validity Runs predictive validity for all groups in the data. data (curvefit.core.data.Data) : data and specifications for the whole analysis group (str) : name of the group model (curvefit.models.base.Model) : a model object that may be copied depending on whether or not in debug mode solver (curvefit.solvers.solver.Solver) : a solver used to fit the model get_residual_data Return the out of sample residuals for all groups from the data argument to run_predictive_validity . Important input to the _ResidualModel for eventually creating uncertainty.","title":"PredictiveValidity"},{"location":"extract_md/PredictiveValidity/#curvefituncertaintypredictive_validity","text":"","title":"curvefit.uncertainty.predictive_validity"},{"location":"extract_md/PredictiveValidity/#out-of-sample-predictive-validity","text":"Runs out of sample predictive validity for all groups in the data with the specified model and solver. Uses a Residuals object for each group to store predictions and compute out of sample residuals at each time point.","title":"Out of sample predictive validity"},{"location":"extract_md/PredictiveValidity/#arguments","text":"evaluation_space (Callable) : the space in which to do predictive validity, can be different than the space that you're fitting the model in debug_mode (bool) : whether or not to store all of the model data and results used for every single group-specific model (saves a lot of memory when debug_mode = False )","title":"Arguments"},{"location":"extract_md/PredictiveValidity/#attributes","text":"theta (int) : the amount of scaling for the residuals relative to the predictions. If the evaluation space is in log space, then theta is set to 0. so that they are absolute residuals. If the evaluation space is in linear space, then theta is set to 1. so that they are relative residuals to the prediction magnitude. `group_residuals (Dict[str: curvefit.uncertainty.residuals.Residual]): residual objects for each group in the dataset `group_records (Dict[str: List[curvefit.solvers.solver.Solver]]): a list of solvers at each time point in the predictive validity analysis for a particular group","title":"Attributes"},{"location":"extract_md/PredictiveValidity/#methods","text":"","title":"Methods"},{"location":"extract_md/PredictiveValidity/#_make_group_info","text":"Creates a ResidualInfo object based on the data passed in and for a particular group. data (curvefit.core.data.DataSpecs) : data specifications group_name (str) : the name of the group","title":"_make_group_info"},{"location":"extract_md/PredictiveValidity/#_run_group_pv","text":"Runs the predictive validity analysis for only one group using the data , model and solver from the run_predictive_validity() function.","title":"_run_group_pv"},{"location":"extract_md/PredictiveValidity/#run_predictive_validity","text":"Runs predictive validity for all groups in the data. data (curvefit.core.data.Data) : data and specifications for the whole analysis group (str) : name of the group model (curvefit.models.base.Model) : a model object that may be copied depending on whether or not in debug mode solver (curvefit.solvers.solver.Solver) : a solver used to fit the model","title":"run_predictive_validity"},{"location":"extract_md/PredictiveValidity/#get_residual_data","text":"Return the out of sample residuals for all groups from the data argument to run_predictive_validity . Important input to the _ResidualModel for eventually creating uncertainty.","title":"get_residual_data"},{"location":"extract_md/PriorInitializer/","text":"curvefit.initializer.initializer.PriorInitializer A prior initializer for a parameter set For a given solver, model and parameter set there are a number of strategies to get information to inform a prior for a later model fit. A PriorInitializer uses one or more PriorInitializerComponent s to inform what priors should be for a new parameter set. Typically, the data passed to PriorInitializer is only a subset of the data, for example a subset that has a substantial number of data points that result in well-informed parameter estimates for all parameters in a model. The updated parameter set that results from PriorInitializer.initialize() has priors that are informed by whatever PriorInitializerComponent s were passed in. These PriorInitializerComponents are model-specification-specific . You need to be aware of what component you are passing in and that it matches your model_prototype. If it does not, an error will be thrown. The requirements are listed in the description sections of each PriorInitializerComponent . Arguments self.prior_initializer_components (List[PriorInitializerComponent]) a list of prior initializer components (instantiated) to use in updating the parameter set Attributes self.component_types (List[str]) : list of the types of initializer components self.joint_solver (curvefit.solvers.solver.Solver) : a solver/model run on all of the data `self.individual_solvers (List[curvefit.solvers.solver.Solver]): a list of solver/models run on each group in the data individually Methods initialize For a given data, model prototype and solver prototype, run the prior initialization for all prior initializer components and return an updated parameter set. data (curvefit.core.data.Data) : a Data object that represents all of the data that will be used in the initialization (this will often be a subset of all available data) model_prototype (curvefit.core.model.CoreModel) : a model that will be used as the prototype and cloned for the joint and individual fits `solver_prototype (curvefit.solvers.solver.Solver): a solver that will be used as the prototype and cloned for the joint and individual fits Usage prior_initializer = PriorInitializer ([ LnAlphaBetaPrior (), BetaPrior ()]) new_parameter_set = prior_initializer . initialize ( data = data , model_prototype = model_prototype , solver_prototype = solver_prototype , parameter_set_prototype )","title":"PriorInitializer"},{"location":"extract_md/PriorInitializer/#curvefitinitializerinitializerpriorinitializer","text":"","title":"curvefit.initializer.initializer.PriorInitializer"},{"location":"extract_md/PriorInitializer/#a-prior-initializer-for-a-parameter-set","text":"For a given solver, model and parameter set there are a number of strategies to get information to inform a prior for a later model fit. A PriorInitializer uses one or more PriorInitializerComponent s to inform what priors should be for a new parameter set. Typically, the data passed to PriorInitializer is only a subset of the data, for example a subset that has a substantial number of data points that result in well-informed parameter estimates for all parameters in a model. The updated parameter set that results from PriorInitializer.initialize() has priors that are informed by whatever PriorInitializerComponent s were passed in. These PriorInitializerComponents are model-specification-specific . You need to be aware of what component you are passing in and that it matches your model_prototype. If it does not, an error will be thrown. The requirements are listed in the description sections of each PriorInitializerComponent .","title":"A prior initializer for a parameter set"},{"location":"extract_md/PriorInitializer/#arguments","text":"self.prior_initializer_components (List[PriorInitializerComponent]) a list of prior initializer components (instantiated) to use in updating the parameter set","title":"Arguments"},{"location":"extract_md/PriorInitializer/#attributes","text":"self.component_types (List[str]) : list of the types of initializer components self.joint_solver (curvefit.solvers.solver.Solver) : a solver/model run on all of the data `self.individual_solvers (List[curvefit.solvers.solver.Solver]): a list of solver/models run on each group in the data individually","title":"Attributes"},{"location":"extract_md/PriorInitializer/#methods","text":"","title":"Methods"},{"location":"extract_md/PriorInitializer/#initialize","text":"For a given data, model prototype and solver prototype, run the prior initialization for all prior initializer components and return an updated parameter set. data (curvefit.core.data.Data) : a Data object that represents all of the data that will be used in the initialization (this will often be a subset of all available data) model_prototype (curvefit.core.model.CoreModel) : a model that will be used as the prototype and cloned for the joint and individual fits `solver_prototype (curvefit.solvers.solver.Solver): a solver that will be used as the prototype and cloned for the joint and individual fits","title":"initialize"},{"location":"extract_md/PriorInitializer/#usage","text":"prior_initializer = PriorInitializer ([ LnAlphaBetaPrior (), BetaPrior ()]) new_parameter_set = prior_initializer . initialize ( data = data , model_prototype = model_prototype , solver_prototype = solver_prototype , parameter_set_prototype )","title":"Usage"},{"location":"extract_md/PriorInitializerComponent/","text":"curvefit.initializer.initializer.PriorInitializerComponent A component of a prior initializer for a model For a given solver, model and parameter set there are a number of strategies to get information to inform a prior for a later model fit. This base class is used to build prior initializer components, that is, objects that extract information from models that have already been run to get well-informed priors for later runs. Prior initializer components can have two types: priors that are extracted from the analysis of a joint model run (one model run with > 1 group), and priors that are extracted from the analysis of a bunch of individual models runs (many models runs with 1 group each). See the documentation on JointPriorInitializerComponent and IndividualPriorInitializerComponent . Attributes self.component_type (str, None) : type of component, overridden in subclasses (\"joint\" or \"individual\") Methods _extract_prior A method to extract prior information from a solver or a list of solvers. Overridden in subclasses. solver (List[Solver], curvefit.core.solver.Solver) : solver or list of solvers that have fit models to data using a parameter set _update_parameter_set A method to update a parameter set given information that was extracted about the priors from the solvers. solver (List[Solver], curvefit.core.solver.Solver) : see above parameter_set_prototype (curvefit.core.parameter.ParameterSet) : a parameter set that will be updated with new prior information","title":"PriorInitializerComponent"},{"location":"extract_md/PriorInitializerComponent/#curvefitinitializerinitializerpriorinitializercomponent","text":"","title":"curvefit.initializer.initializer.PriorInitializerComponent"},{"location":"extract_md/PriorInitializerComponent/#a-component-of-a-prior-initializer-for-a-model","text":"For a given solver, model and parameter set there are a number of strategies to get information to inform a prior for a later model fit. This base class is used to build prior initializer components, that is, objects that extract information from models that have already been run to get well-informed priors for later runs. Prior initializer components can have two types: priors that are extracted from the analysis of a joint model run (one model run with > 1 group), and priors that are extracted from the analysis of a bunch of individual models runs (many models runs with 1 group each). See the documentation on JointPriorInitializerComponent and IndividualPriorInitializerComponent .","title":"A component of a prior initializer for a model"},{"location":"extract_md/PriorInitializerComponent/#attributes","text":"self.component_type (str, None) : type of component, overridden in subclasses (\"joint\" or \"individual\")","title":"Attributes"},{"location":"extract_md/PriorInitializerComponent/#methods","text":"","title":"Methods"},{"location":"extract_md/PriorInitializerComponent/#_extract_prior","text":"A method to extract prior information from a solver or a list of solvers. Overridden in subclasses. solver (List[Solver], curvefit.core.solver.Solver) : solver or list of solvers that have fit models to data using a parameter set","title":"_extract_prior"},{"location":"extract_md/PriorInitializerComponent/#_update_parameter_set","text":"A method to update a parameter set given information that was extracted about the priors from the solvers. solver (List[Solver], curvefit.core.solver.Solver) : see above parameter_set_prototype (curvefit.core.parameter.ParameterSet) : a parameter set that will be updated with new prior information","title":"_update_parameter_set"},{"location":"extract_md/Prototype/","text":"curvefit.core.prototype.Prototype Base class for an object that can be cloned Some objects in the curvefit package need to be treated as prototypes and cloned. If an object needs this ability, it will be a subclass of Prototype and inherit the clone method. Methods clone Returns a deepcopy of itself.","title":"Prototype"},{"location":"extract_md/Prototype/#curvefitcoreprototypeprototype","text":"","title":"curvefit.core.prototype.Prototype"},{"location":"extract_md/Prototype/#base-class-for-an-object-that-can-be-cloned","text":"Some objects in the curvefit package need to be treated as prototypes and cloned. If an object needs this ability, it will be a subclass of Prototype and inherit the clone method.","title":"Base class for an object that can be cloned"},{"location":"extract_md/Prototype/#methods","text":"","title":"Methods"},{"location":"extract_md/Prototype/#clone","text":"Returns a deepcopy of itself.","title":"clone"},{"location":"extract_md/ResidualInfo/","text":"curvefit.uncertainty.predictive_validity.ResidualInfo Keeps track of metadata about the residuals Arguments group_name (str) : name of the group times (np.array) : times that have data obs (np.array) : observations at times Attributes num_times (int) : number of times difference (np.array) : array of length num_times - 1 that represents the sequential differences in each time to the next amount_data (np.array) : the amount of data at each time point","title":"ResidualInfo"},{"location":"extract_md/ResidualInfo/#curvefituncertaintypredictive_validityresidualinfo","text":"","title":"curvefit.uncertainty.predictive_validity.ResidualInfo"},{"location":"extract_md/ResidualInfo/#keeps-track-of-metadata-about-the-residuals","text":"","title":"Keeps track of metadata about the residuals"},{"location":"extract_md/ResidualInfo/#arguments","text":"group_name (str) : name of the group times (np.array) : times that have data obs (np.array) : observations at times","title":"Arguments"},{"location":"extract_md/ResidualInfo/#attributes","text":"num_times (int) : number of times difference (np.array) : array of length num_times - 1 that represents the sequential differences in each time to the next amount_data (np.array) : the amount of data at each time point","title":"Attributes"},{"location":"extract_md/Residuals/","text":"curvefit.uncertainty.predictive_validity.residuals Data storage and manipulation for residual matrices The Residuals class keeps track of a prediction matrix and the associated residual matrix at each time point. Arguments residual_info (ResidualInfo) : metadata about residuals data_specs (curvefit.core.data.DataSpecs) : specifications about what data was passed in in order to generate these residuals Attributes prediction_matrix (np.ndarray) : square matrix of size total number of time points for a group. The rows of the matrix are predictions from models fit on progressively more data, and the columns of the matrix are the predictions for each point in the time series. Everything above the diagonal is an out of sample prediction. residual_matrix (np.ndarray) : square matrix of the same size as prediction matrix but has had observations subtracted off of it and (potentially) scaled by the prediction value Methods _record_predictions Records a set of predictions into the prediction matrix. i (int) : the ith set of predictions (the whole time series) to record predictions (np.array) : 1d numpy array of predictions across the time series _compute_residuals Given some observed data and an amount of scaling (theta), compute the residuals. obs (np.array) : 1d numpy array of observed data in the same space as the predictions theta (float) : amount of scaling. A theta = 1 means that they are relative residuals (relative to the prediction magnitude) and a theta = 0 means that they are absolute residuals _condense_matrix Takes a square matrix of predictions or residuals and condenses this to a smaller matrix that only has out of sample predictions or residuals, and matches it to metadata about those residuals or predictions including how much data was used to predict (data_density --> \"num_data\") and how far out was this prediction time point from the last observed time point (sequential diffs --> \"far_out\"). matrix (np.ndarray) : the square matrix to condense sequential_diffs (np.array) : 1d array of sequential differences in time between observations (e.g. observations 3 time points apart would have sequential_diffs = 3) (results in the \"far_out\" column) data_density (np.array) : the amount of data at each time point dropping all observations beyond this time point (results in the \"num_data\" column)","title":"Residuals"},{"location":"extract_md/Residuals/#curvefituncertaintypredictive_validityresiduals","text":"","title":"curvefit.uncertainty.predictive_validity.residuals"},{"location":"extract_md/Residuals/#data-storage-and-manipulation-for-residual-matrices","text":"The Residuals class keeps track of a prediction matrix and the associated residual matrix at each time point.","title":"Data storage and manipulation for residual matrices"},{"location":"extract_md/Residuals/#arguments","text":"residual_info (ResidualInfo) : metadata about residuals data_specs (curvefit.core.data.DataSpecs) : specifications about what data was passed in in order to generate these residuals","title":"Arguments"},{"location":"extract_md/Residuals/#attributes","text":"prediction_matrix (np.ndarray) : square matrix of size total number of time points for a group. The rows of the matrix are predictions from models fit on progressively more data, and the columns of the matrix are the predictions for each point in the time series. Everything above the diagonal is an out of sample prediction. residual_matrix (np.ndarray) : square matrix of the same size as prediction matrix but has had observations subtracted off of it and (potentially) scaled by the prediction value","title":"Attributes"},{"location":"extract_md/Residuals/#methods","text":"","title":"Methods"},{"location":"extract_md/Residuals/#_record_predictions","text":"Records a set of predictions into the prediction matrix. i (int) : the ith set of predictions (the whole time series) to record predictions (np.array) : 1d numpy array of predictions across the time series","title":"_record_predictions"},{"location":"extract_md/Residuals/#_compute_residuals","text":"Given some observed data and an amount of scaling (theta), compute the residuals. obs (np.array) : 1d numpy array of observed data in the same space as the predictions theta (float) : amount of scaling. A theta = 1 means that they are relative residuals (relative to the prediction magnitude) and a theta = 0 means that they are absolute residuals","title":"_compute_residuals"},{"location":"extract_md/Residuals/#_condense_matrix","text":"Takes a square matrix of predictions or residuals and condenses this to a smaller matrix that only has out of sample predictions or residuals, and matches it to metadata about those residuals or predictions including how much data was used to predict (data_density --> \"num_data\") and how far out was this prediction time point from the last observed time point (sequential diffs --> \"far_out\"). matrix (np.ndarray) : the square matrix to condense sequential_diffs (np.array) : 1d array of sequential differences in time between observations (e.g. observations 3 time points apart would have sequential_diffs = 3) (results in the \"far_out\" column) data_density (np.array) : the amount of data at each time point dropping all observations beyond this time point (results in the \"num_data\" column)","title":"_condense_matrix"},{"location":"extract_md/ScipyOpt/","text":"curvefit.solvers.solvers.ScipyOpt Scipy Optimize Solver Use scipy optimize to fit the model using the L-BFGS-B method. See Solver for arguments and methods.","title":"ScipyOpt"},{"location":"extract_md/ScipyOpt/#curvefitsolverssolversscipyopt","text":"","title":"curvefit.solvers.solvers.ScipyOpt"},{"location":"extract_md/ScipyOpt/#scipy-optimize-solver","text":"Use scipy optimize to fit the model using the L-BFGS-B method. See Solver for arguments and methods.","title":"Scipy Optimize Solver"},{"location":"extract_md/SmartInitialization/","text":"curvefit.solvers.solvers.SmartInitialization SmartInitialization Solver Built on top of any solver; used when there are many groups that will be fit to using random effects. First fits individual models for each group, and uses their fixed effect values minus the fixed effects mean across all groups as the random effects initial values (finding a \"smart\" initial value for better convergence). See CompositeSolver for more arguments and methods.","title":"SmartInitialization"},{"location":"extract_md/SmartInitialization/#curvefitsolverssolverssmartinitialization","text":"","title":"curvefit.solvers.solvers.SmartInitialization"},{"location":"extract_md/SmartInitialization/#smartinitialization-solver","text":"Built on top of any solver; used when there are many groups that will be fit to using random effects. First fits individual models for each group, and uses their fixed effect values minus the fixed effects mean across all groups as the random effects initial values (finding a \"smart\" initial value for better convergence). See CompositeSolver for more arguments and methods.","title":"SmartInitialization Solver"},{"location":"extract_md/SmoothResidualModel/","text":"curvefit.core.residual_model.SmoothResidualModel A local smoother for the coefficient of variation in forecasts This is a residual model (see _ResidualModel for a description). This particular residual model creates a smoothed standard deviation over the residual data. It calculates the standard deviation of the residuals with a moving window over neighboring covariate values. The specific covariates for this residual model are num_data and far_out: how much data did the prediction model have and how far out into the future was it predicting. To extrapolate to unobserved values of the covariates in order to predict the residuals for those observations, it prioritizes num_data and then far_out in a simple \"carry forward\" extrapolation. Syntax d = SmoothResidualModel ( cv_bounds , covariates , exclude_groups , num_smooth_iterations , smooth_radius , robust ) Arguments cv_bounds (List[float]) : a 2-element list of bounds on the coefficient of variation. The first element is the lower bound and the second is the upper bound covariates (Dict[str: None, str]) : a dictionary of covariates to use in the model. The keys of the dictionary are the names of covariates to include in the residual model fitting and the optional values for each key are the subsets of the covariates to use (e.g. only use the subset of data where covariate1 > 10. in the fitting). NOTE : this residual model requires that only two covariates, \"far_out\" and \"num_data\" are used. exclude_groups (optional, List[str]) : a list of groups to exclude from the fitting process (not excluded from making predictions) num_smooth_iterations (int) : how many times should the model smooth over the residual matrix. If 1, then only calculates the standard deviation over a moving window. If > 1, then runs a local smoother over the standard deviation surface num_smooth_iterations - 1 times. smooth_radius (List[int]) : the size of the moving window in each of the covariate directions. Since for this residual model only two covariates are used, this needs to be a 2-element list of integers specifying how many units of each covariate to consider for the neighborhood. For example, if covariates is a dictionary with ['far_out', 'num_data'] as the keys, and smooth_radius=[2, 2] , then the standard deviation for the residual with far_out == 4 and num_data == 3 will be calculated over the window 2 < far_out < 6 and 1 < num_data < 5 . robust (bool) : whether or not to use a robust estimator for the standard deviation (1.4826 * median absolute deviation). Attributes self.smoothed_residual_data (np.array) : a smooth surface of residual standard deviations across the covariate axes, only populated after fit_residuals() is called self.covariate_names (List[str]) : list of covariate names (keys of self.covariates ) Methods See _ResidualModel for descriptions of the class methods for a _ResidualModel. Usage SmoothResidualModel ( cv_bounds = [ 0. , np . inf ], covariates = { 'far_out' : 'far_out >= 10' , 'num_data' : None }, exclude_groups = None , num_smooth_iterations = 1 , smooth_radius = [ 1 , 1 ], robust = True )","title":"SmoothResidualModel"},{"location":"extract_md/SmoothResidualModel/#curvefitcoreresidual_modelsmoothresidualmodel","text":"","title":"curvefit.core.residual_model.SmoothResidualModel"},{"location":"extract_md/SmoothResidualModel/#a-local-smoother-for-the-coefficient-of-variation-in-forecasts","text":"This is a residual model (see _ResidualModel for a description). This particular residual model creates a smoothed standard deviation over the residual data. It calculates the standard deviation of the residuals with a moving window over neighboring covariate values. The specific covariates for this residual model are num_data and far_out: how much data did the prediction model have and how far out into the future was it predicting. To extrapolate to unobserved values of the covariates in order to predict the residuals for those observations, it prioritizes num_data and then far_out in a simple \"carry forward\" extrapolation.","title":"A local smoother for the coefficient of variation in forecasts"},{"location":"extract_md/SmoothResidualModel/#syntax","text":"d = SmoothResidualModel ( cv_bounds , covariates , exclude_groups , num_smooth_iterations , smooth_radius , robust )","title":"Syntax"},{"location":"extract_md/SmoothResidualModel/#arguments","text":"cv_bounds (List[float]) : a 2-element list of bounds on the coefficient of variation. The first element is the lower bound and the second is the upper bound covariates (Dict[str: None, str]) : a dictionary of covariates to use in the model. The keys of the dictionary are the names of covariates to include in the residual model fitting and the optional values for each key are the subsets of the covariates to use (e.g. only use the subset of data where covariate1 > 10. in the fitting). NOTE : this residual model requires that only two covariates, \"far_out\" and \"num_data\" are used. exclude_groups (optional, List[str]) : a list of groups to exclude from the fitting process (not excluded from making predictions) num_smooth_iterations (int) : how many times should the model smooth over the residual matrix. If 1, then only calculates the standard deviation over a moving window. If > 1, then runs a local smoother over the standard deviation surface num_smooth_iterations - 1 times. smooth_radius (List[int]) : the size of the moving window in each of the covariate directions. Since for this residual model only two covariates are used, this needs to be a 2-element list of integers specifying how many units of each covariate to consider for the neighborhood. For example, if covariates is a dictionary with ['far_out', 'num_data'] as the keys, and smooth_radius=[2, 2] , then the standard deviation for the residual with far_out == 4 and num_data == 3 will be calculated over the window 2 < far_out < 6 and 1 < num_data < 5 . robust (bool) : whether or not to use a robust estimator for the standard deviation (1.4826 * median absolute deviation).","title":"Arguments"},{"location":"extract_md/SmoothResidualModel/#attributes","text":"self.smoothed_residual_data (np.array) : a smooth surface of residual standard deviations across the covariate axes, only populated after fit_residuals() is called self.covariate_names (List[str]) : list of covariate names (keys of self.covariates )","title":"Attributes"},{"location":"extract_md/SmoothResidualModel/#methods","text":"See _ResidualModel for descriptions of the class methods for a _ResidualModel.","title":"Methods"},{"location":"extract_md/SmoothResidualModel/#usage","text":"SmoothResidualModel ( cv_bounds = [ 0. , np . inf ], covariates = { 'far_out' : 'far_out >= 10' , 'num_data' : None }, exclude_groups = None , num_smooth_iterations = 1 , smooth_radius = [ 1 , 1 ], robust = True )","title":"Usage"},{"location":"extract_md/Solver/","text":"curvefit.solvers.solvers.Solver Solver base class that fits a Model In order to fit a curvefit.models.base.Model , you must first define a Solver and assign the model to the solver. The reason for this is that there might be multiple ways that you could solve a particular model. Arguments model_instance (curvefit.models.base.Model) : the model instance that will be solved Methods fit Fit the solver to some data using self.model_instance . data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object options (None | Options) : an optional Options object that has fit specifications for the underlying solver; overrides the options that have already been set predict Create predictions based on the optimal values estimated by the solver. **kwargs : keyword arguments passed to self.model_instance.predict() set_options Set a dictionary of options that will be used in the optimization. set_model_instance Attach a new model instance. detach_model_instance Detach the current model instance. get_model_instance Get the current model instance.","title":"Solver"},{"location":"extract_md/Solver/#curvefitsolverssolverssolver","text":"","title":"curvefit.solvers.solvers.Solver"},{"location":"extract_md/Solver/#solver-base-class-that-fits-a-model","text":"In order to fit a curvefit.models.base.Model , you must first define a Solver and assign the model to the solver. The reason for this is that there might be multiple ways that you could solve a particular model.","title":"Solver base class that fits a Model"},{"location":"extract_md/Solver/#arguments","text":"model_instance (curvefit.models.base.Model) : the model instance that will be solved","title":"Arguments"},{"location":"extract_md/Solver/#methods","text":"","title":"Methods"},{"location":"extract_md/Solver/#fit","text":"Fit the solver to some data using self.model_instance . data (Tuple[pd.DataFrame, DataSpecs]) : the input data frame to be fit, and data specifications object options (None | Options) : an optional Options object that has fit specifications for the underlying solver; overrides the options that have already been set","title":"fit"},{"location":"extract_md/Solver/#predict","text":"Create predictions based on the optimal values estimated by the solver. **kwargs : keyword arguments passed to self.model_instance.predict()","title":"predict"},{"location":"extract_md/Solver/#set_options","text":"Set a dictionary of options that will be used in the optimization.","title":"set_options"},{"location":"extract_md/Solver/#set_model_instance","text":"Attach a new model instance.","title":"set_model_instance"},{"location":"extract_md/Solver/#detach_model_instance","text":"Detach the current model instance.","title":"detach_model_instance"},{"location":"extract_md/Solver/#get_model_instance","text":"Get the current model instance.","title":"get_model_instance"},{"location":"extract_md/Variable/","text":"curvefit.core.parameter.Variable A variable to be estimated during the fit A Variable is the most detailed unit to be estimated for curve fitting. A Variable corresponds to an effect on a parameter -- and can contain a combination of both a fixed effect and random effects. A Variable needs a \"covariate\" , but the covariate in the data can be just a column of 1's, in which case a Variable is equivalent to a Parameter . If instead the values of the \"covariate\" argument differ for different rows of the data, Variable multiplies the covariate to get the Parameter for that data row. A curvefit model is made up of multiple parameters. For more information, see Parameter and ParameterSet . Arguments covariate (str) : name of the covariate for this variable (corresponds to what it will be in the data that is eventually used to fit the model) var_link_fun (Callable) : link function for the variable fe_init (float) : initial value to be used in the optimization for the fixed effect re_init (float) : initial value to be used in the optimization for the random effect re_zero_sum_std (float) : standard deviation of the zero sum prior for he random effects corresponding to this variable. fe_gprior (optional, List[float]) : list of Gaussian priors the fixed effect where the first element is the prior mean and the second element is the prior standard deviation re_gprior (optional, List[float]) : list of Gaussian priors the random effect where the first element is the prior mean and the second element is the prior standard deviation fe_bounds (optional, List[float]) : list of box constraints for the fixed effects during the optimization where the first element is the lower bound and the second element is the upper bound re_bounds (optional, List[float]) : list of box constraints for the fixed effects during the optimization where the first element is the lower bound and the second element is the upper bound Usage from curvefit.core.parameter import Variable var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. )","title":"Variable"},{"location":"extract_md/Variable/#curvefitcoreparametervariable","text":"","title":"curvefit.core.parameter.Variable"},{"location":"extract_md/Variable/#a-variable-to-be-estimated-during-the-fit","text":"A Variable is the most detailed unit to be estimated for curve fitting. A Variable corresponds to an effect on a parameter -- and can contain a combination of both a fixed effect and random effects. A Variable needs a \"covariate\" , but the covariate in the data can be just a column of 1's, in which case a Variable is equivalent to a Parameter . If instead the values of the \"covariate\" argument differ for different rows of the data, Variable multiplies the covariate to get the Parameter for that data row. A curvefit model is made up of multiple parameters. For more information, see Parameter and ParameterSet .","title":"A variable to be estimated during the fit"},{"location":"extract_md/Variable/#arguments","text":"covariate (str) : name of the covariate for this variable (corresponds to what it will be in the data that is eventually used to fit the model) var_link_fun (Callable) : link function for the variable fe_init (float) : initial value to be used in the optimization for the fixed effect re_init (float) : initial value to be used in the optimization for the random effect re_zero_sum_std (float) : standard deviation of the zero sum prior for he random effects corresponding to this variable. fe_gprior (optional, List[float]) : list of Gaussian priors the fixed effect where the first element is the prior mean and the second element is the prior standard deviation re_gprior (optional, List[float]) : list of Gaussian priors the random effect where the first element is the prior mean and the second element is the prior standard deviation fe_bounds (optional, List[float]) : list of box constraints for the fixed effects during the optimization where the first element is the lower bound and the second element is the upper bound re_bounds (optional, List[float]) : list of box constraints for the fixed effects during the optimization where the first element is the lower bound and the second element is the upper bound","title":"Arguments"},{"location":"extract_md/Variable/#usage","text":"from curvefit.core.parameter import Variable var = Variable ( covariate = 'ones' , var_link_fun = lambda x : x , fe_init = 0. , re_init = 0. )","title":"Usage"},{"location":"extract_md/_ResidualModel/","text":"curvefit.core.residual_model._ResidualModel A model for describing the out of sample residuals This is a model that describes how the out of sample residuals vary with a set of covariates. The goal is to be able to understand what the residuals moving into the future will look like based on key covariates (e.g. how much data do we currently have, how far out are we trying to predict, etc.). Ultimately this class simulates residuals into the future. It does this in coefficient of variation space (if predicting in log space --> absolute residuals; if predicting in linear space --> relative residuals). This is the BASE class and should not be used directly. It should be subclassed and the methods overwritten based on the type of residual model. See the subclasses descriptions for each of their methods including SmoothResidualModel . Arguments cv_bounds (List[float]) : a 2-element list of bounds on the coefficient of variation. The first element is the lower bound and the second is the upper bound covariates (Dict[str: None, str]) : a dictionary of covariates to use in the model. The keys of the dictionary are the names of covariates to include in the residual model fitting and the optional values for each key are the subsets of the covariates to use (e.g. only use the subset of data where covariate1 > 10. in the fitting). exclude_groups (List[str]) : a list of groups to exclude from the fitting process (not excluded from making predictions) Methods fit_residuals Fits the residual model to the residual data frame passed in. residual_df (pd.DataFrame) : a data frame that contains all of the covariates and a residual observation that will be used for fitting the model (the design matrix) simulate_residuals Simulates residuals from the fitted residual model for particular covariate values. Returns an array of simulated residuals of size ( num_simulations , num_covs ) where num_covs is the product of the length of the values in covariate_specs (get this by doing _ResidualModel._expand_grid to get all covariate value combinations). covariate_specs (Dict[str: np.array]) : a dictionary of covariate values to create residual simulations for num_simulations (int) : number of residual simulations to produce","title":"_ResidualModel"},{"location":"extract_md/_ResidualModel/#curvefitcoreresidual_model_residualmodel","text":"","title":"curvefit.core.residual_model._ResidualModel"},{"location":"extract_md/_ResidualModel/#a-model-for-describing-the-out-of-sample-residuals","text":"This is a model that describes how the out of sample residuals vary with a set of covariates. The goal is to be able to understand what the residuals moving into the future will look like based on key covariates (e.g. how much data do we currently have, how far out are we trying to predict, etc.). Ultimately this class simulates residuals into the future. It does this in coefficient of variation space (if predicting in log space --> absolute residuals; if predicting in linear space --> relative residuals). This is the BASE class and should not be used directly. It should be subclassed and the methods overwritten based on the type of residual model. See the subclasses descriptions for each of their methods including SmoothResidualModel .","title":"A model for describing the out of sample residuals"},{"location":"extract_md/_ResidualModel/#arguments","text":"cv_bounds (List[float]) : a 2-element list of bounds on the coefficient of variation. The first element is the lower bound and the second is the upper bound covariates (Dict[str: None, str]) : a dictionary of covariates to use in the model. The keys of the dictionary are the names of covariates to include in the residual model fitting and the optional values for each key are the subsets of the covariates to use (e.g. only use the subset of data where covariate1 > 10. in the fitting). exclude_groups (List[str]) : a list of groups to exclude from the fitting process (not excluded from making predictions)","title":"Arguments"},{"location":"extract_md/_ResidualModel/#methods","text":"fit_residuals Fits the residual model to the residual data frame passed in. residual_df (pd.DataFrame) : a data frame that contains all of the covariates and a residual observation that will be used for fitting the model (the design matrix) simulate_residuals Simulates residuals from the fitted residual model for particular covariate values. Returns an array of simulated residuals of size ( num_simulations , num_covs ) where num_covs is the product of the length of the values in covariate_specs (get this by doing _ResidualModel._expand_grid to get all covariate value combinations). covariate_specs (Dict[str: np.array]) : a dictionary of covariate values to create residual simulations for num_simulations (int) : number of residual simulations to produce","title":"Methods"},{"location":"extract_md/convolve_sum/","text":"curvefit.utils.smoothing.convolve_sum Convolve sum a 2D matrix by given radius. Arguments mat (numpy.ndarray) : matrix of interest radius (array-like{int} | None, optional) : given radius, if None assume radius = (0, 0) Returns numpy.ndarray : the convolved sum, with the same shape with original matrix.","title":"convolve_sum"},{"location":"extract_md/convolve_sum/#curvefitutilssmoothingconvolve_sum","text":"","title":"curvefit.utils.smoothing.convolve_sum"},{"location":"extract_md/convolve_sum/#convolve-sum-a-2d-matrix-by-given-radius","text":"","title":"Convolve sum a 2D matrix by given radius."},{"location":"extract_md/convolve_sum/#arguments","text":"mat (numpy.ndarray) : matrix of interest radius (array-like{int} | None, optional) : given radius, if None assume radius = (0, 0)","title":"Arguments"},{"location":"extract_md/convolve_sum/#returns","text":"numpy.ndarray : the convolved sum, with the same shape with original matrix.","title":"Returns"},{"location":"extract_md/covariate_xam/","text":"Using Covariates Generalized Gaussian Cumulative Distribution Function The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate. Fixed Effects We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate. Random effects For this example the random effects are constrained to be zero. Social Distance For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three. Time Grid A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T . Measurement Values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import sys import pandas import numpy import scipy from curvefit.core.functions import gaussian_cdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # link function used for beta def identity_fun ( x ): return x # link function used for alpha, p def exp_fun ( x ): return numpy . exp ( x ) # inverse of function used for alpha, p def log_fun ( x ): return numpy . log ( x ) # true value for fixed effects fe_true = numpy . array ([ a_true , b_true , c_true , phi_true ]) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_value = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ): social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_value [ i ] = [ alpha_true , beta_true , p_true ] params_value = numpy . transpose ( params_value ) measurement_value = gaussian_cdf ( independent_var , params_value ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = [ 'cov_one' , 'social_distance' ], col_group = 'data_group' , obs_space = gaussian_cdf , col_obs_se = 'measurement_std' ) a_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = a_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) b_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = b_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) b_social_distance = Variable ( covariate = 'social_distance' , var_link_fun = identity_fun , fe_init = b_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) phi_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = p_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = exp_fun , variables = [ a_intercept ]) beta = Parameter ( param_name = 'beta' , link_fun = identity_fun , variables = [ b_intercept , b_social_distance ]) p = Parameter ( param_name = 'p' , link_fun = exp_fun , variables = [ phi_intercept ]) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } model = CoreModel ( param_set = parameters , curve_fun = gaussian_cdf , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = True ) # ------------------------------------------------------------------------- # check result for i in range ( 3 ): assert numpy . allclose ( params_estimate [ i ], params_value [ i ], rtol = rel_tol ) print ( 'covariate.py: OK' )","title":"covariate_xam"},{"location":"extract_md/covariate_xam/#using-covariates","text":"","title":"Using Covariates"},{"location":"extract_md/covariate_xam/#generalized-gaussian-cumulative-distribution-function","text":"The model for the mean of the data for this example is: f(t; \\alpha, \\beta, p) = \\frac{p}{2} \\left( 1 + \\frac{2}{\\pi} \\int_0^{\\alpha ( t - \\beta )} \\exp( - \\tau^2 ) d \\tau \\right) where \\alpha , \\beta , and p are unknown parameters. In addition, the value of \\beta depends on covariate.","title":"Generalized Gaussian Cumulative Distribution Function"},{"location":"extract_md/covariate_xam/#fixed-effects","text":"We use the notation a , b , c and \\phi for the fixed effects corresponding to the parameters \\alpha , \\beta , and p . For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b + c \\cdot s \\\\ p & = \\exp( \\phi ) \\end{aligned} where s is the social distance covariate.","title":"Fixed Effects"},{"location":"extract_md/covariate_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/covariate_xam/#social-distance","text":"For this simulation, the social distance covariate has two values: s_i = \\left\\{ \\begin{array}{ll} 0 & \\mbox{if} \\; i < n_D / 2 \\\\ 1 & \\mbox{otherwise} \\end{array} \\right.","title":"Social Distance"},{"location":"extract_md/covariate_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/covariate_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import math n_data = 21 # number simulated measurements to generate b_true = 20.0 # b used to simulate data a_true = math . log ( 2.0 / b_true ) # a used to simulate data c_true = 1.0 / b_true # c used to simulate data phi_true = math . log ( 0.1 ) # phi used to simulate data rel_tol = 1e-5 # relative tolerance used to check optimal solution The fixed effects a , b , c , and \\phi are initialized so that they correspond to the true fixed effects divided by three.","title":"Problem Settings"},{"location":"extract_md/covariate_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = b_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is b_T .","title":"Time Grid"},{"location":"extract_md/covariate_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , b_T + c_T \\cdot s_i , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement Values"},{"location":"extract_md/covariate_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import sys import pandas import numpy import scipy from curvefit.core.functions import gaussian_cdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # link function used for beta def identity_fun ( x ): return x # link function used for alpha, p def exp_fun ( x ): return numpy . exp ( x ) # inverse of function used for alpha, p def log_fun ( x ): return numpy . log ( x ) # true value for fixed effects fe_true = numpy . array ([ a_true , b_true , c_true , phi_true ]) num_fe = len ( fe_true ) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * b_true / ( n_data - 1 ) social_distance = numpy . zeros ( n_data , dtype = float ) params_value = numpy . zeros (( n_data , 3 ), dtype = float ) alpha_true = numpy . exp ( a_true ) p_true = numpy . exp ( phi_true ) for i in range ( n_data ): social_distance [ i ] = 0 if i < n_data / 2.0 else 1 beta_true = b_true + c_true * social_distance [ i ] params_value [ i ] = [ alpha_true , beta_true , p_true ] params_value = numpy . transpose ( params_value ) measurement_value = gaussian_cdf ( independent_var , params_value ) measurement_std = n_data * [ 0.1 ] cov_one = n_data * [ 1.0 ] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'cov_one' : cov_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = [ 'cov_one' , 'social_distance' ], col_group = 'data_group' , obs_space = gaussian_cdf , col_obs_se = 'measurement_std' ) a_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = a_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) b_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = b_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) b_social_distance = Variable ( covariate = 'social_distance' , var_link_fun = identity_fun , fe_init = b_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) phi_intercept = Variable ( covariate = 'cov_one' , var_link_fun = identity_fun , fe_init = p_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = exp_fun , variables = [ a_intercept ]) beta = Parameter ( param_name = 'beta' , link_fun = identity_fun , variables = [ b_intercept , b_social_distance ]) p = Parameter ( param_name = 'p' , link_fun = exp_fun , variables = [ phi_intercept ]) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } model = CoreModel ( param_set = parameters , curve_fun = gaussian_cdf , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = True ) # ------------------------------------------------------------------------- # check result for i in range ( 3 ): assert numpy . allclose ( params_estimate [ i ], params_value [ i ], rtol = rel_tol ) print ( 'covariate.py: OK' )","title":"Example Source Code"},{"location":"extract_md/data_translator/","text":"curvefit.utils.data.data_translator Translate data from one space to another, only for Gaussian-family functions Arguments data (np.ndarray) : data matrix or vector input_space (str | callable) : input data space. output_space (str | callable) : output data space. threshold (float, optional) : threshold for numbers below 0 in linear space. Returns np.ndarray : translated data.","title":"data_translator"},{"location":"extract_md/data_translator/#curvefitutilsdatadata_translator","text":"","title":"curvefit.utils.data.data_translator"},{"location":"extract_md/data_translator/#translate-data-from-one-space-to-another-only-for-gaussian-family-functions","text":"","title":"Translate data from one space to another, only for Gaussian-family functions"},{"location":"extract_md/data_translator/#arguments","text":"data (np.ndarray) : data matrix or vector input_space (str | callable) : input data space. output_space (str | callable) : output data space. threshold (float, optional) : threshold for numbers below 0 in linear space.","title":"Arguments"},{"location":"extract_md/data_translator/#returns","text":"np.ndarray : translated data.","title":"Returns"},{"location":"extract_md/df_to_mat/","text":"curvefit.utils.smoothing.df_to_mat Convert columns in data frame to matrix Arguments df (pandas.DataFrame) : given data frame. col_val (str) : value column. col_axis (List[str]) : axis column. return_indices (bool, optional) : if True, return indices of the original values and the corresponding axis values in the data frame. Returns (numpy.ndarray) : converted matrix","title":"df_to_mat"},{"location":"extract_md/df_to_mat/#curvefitutilssmoothingdf_to_mat","text":"","title":"curvefit.utils.smoothing.df_to_mat"},{"location":"extract_md/df_to_mat/#convert-columns-in-data-frame-to-matrix","text":"","title":"Convert columns in data frame to matrix"},{"location":"extract_md/df_to_mat/#arguments","text":"df (pandas.DataFrame) : given data frame. col_val (str) : value column. col_axis (List[str]) : axis column. return_indices (bool, optional) : if True, return indices of the original values and the corresponding axis values in the data frame.","title":"Arguments"},{"location":"extract_md/df_to_mat/#returns","text":"(numpy.ndarray) : converted matrix","title":"Returns"},{"location":"extract_md/effects2params/","text":"curvefit.core.effects2params Map Vector of Fixed and Random Effects to Parameter Matrix Extracts fixed and random effects and converts them to parameters. Needs to use unzip_x . Syntax params = curvefit . core . effects2params . effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand = True ) Arguments x (np.array) : This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. group_sizes (array-like) : A vector of positive integers. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes. covs (List[np.ndarray]) : Is a list with length equal to the number of parameters and covs[k] is a two dimensional numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. link_fun (List[Callable]) : The value len(link_fun) is equal to the number of parameters and link_fun[k]` is a function with one numpy array argument and result that acts element by element and transforms the k-th parameter. var_link_fun (List[Callable]) : The value len(var_link_fun) is equal to the number of fixed effects and link_fun[j] is a function with one numpy array argument and result that that acts element by element and transforms the j-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1])` fixed effects correspond to the second parameter and so on. expand (bool) : If expand is True ( False ), create parameters for each observation (for each group of observations). Returns params (array-like) : Let f_j be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the matrix, with row dimension equal the number of groups and column dimension equal the number of fixed effects v_{i,j} = V_j \\left( f_j + r_{i,j} \\right) where V_j is the function var_link_fun[i] . If expand is true (false) i indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a two dimensional numpy array with params.shape[0] equal to the number of parameters and params.shape[1] equal to the number of observations, if expand is true, number of groups, if expand is false. The value params[k][i] is P_k \\left( \\sum_{j(k)} v_j c_{i,j} \\right) where P_k is the function link_fun[k] , j(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the j-th fixed effect and the i-th observation, if expand is true, or i-th group, if expand is false. Example See effects2params_xam .","title":"effects2params"},{"location":"extract_md/effects2params/#curvefitcoreeffects2params","text":"","title":"curvefit.core.effects2params"},{"location":"extract_md/effects2params/#map-vector-of-fixed-and-random-effects-to-parameter-matrix","text":"Extracts fixed and random effects and converts them to parameters. Needs to use unzip_x .","title":"Map Vector of Fixed and Random Effects to Parameter Matrix"},{"location":"extract_md/effects2params/#syntax","text":"params = curvefit . core . effects2params . effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand = True )","title":"Syntax"},{"location":"extract_md/effects2params/#arguments","text":"x (np.array) : This is a one dimensional numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. group_sizes (array-like) : A vector of positive integers. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The total number of observations is the sum of the group sizes. covs (List[np.ndarray]) : Is a list with length equal to the number of parameters and covs[k] is a two dimensional numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. link_fun (List[Callable]) : The value len(link_fun) is equal to the number of parameters and link_fun[k]` is a function with one numpy array argument and result that acts element by element and transforms the k-th parameter. var_link_fun (List[Callable]) : The value len(var_link_fun) is equal to the number of fixed effects and link_fun[j] is a function with one numpy array argument and result that that acts element by element and transforms the j-th fixed effect. The first len(covs[0]) fixed effects correspond to the first parameter, the next len(covs[1])` fixed effects correspond to the second parameter and so on. expand (bool) : If expand is True ( False ), create parameters for each observation (for each group of observations).","title":"Arguments"},{"location":"extract_md/effects2params/#returns","text":"params (array-like) : Let f_j be the vector of fixed effects and r_{i,j} the matrix of random effects corresponding to x . We define the matrix, with row dimension equal the number of groups and column dimension equal the number of fixed effects v_{i,j} = V_j \\left( f_j + r_{i,j} \\right) where V_j is the function var_link_fun[i] . If expand is true (false) i indexes observations (groups). (If expand is true the random effect for a group gets repeated for all the observations in the group.) The return value params is a two dimensional numpy array with params.shape[0] equal to the number of parameters and params.shape[1] equal to the number of observations, if expand is true, number of groups, if expand is false. The value params[k][i] is P_k \\left( \\sum_{j(k)} v_j c_{i,j} \\right) where P_k is the function link_fun[k] , j(k) is the set of fixed effects indices corresponding to the k-th parameter, c_{i,j} is the covariate value corresponding to the j-th fixed effect and the i-th observation, if expand is true, or i-th group, if expand is false.","title":"Returns"},{"location":"extract_md/effects2params/#example","text":"See effects2params_xam .","title":"Example"},{"location":"extract_md/effects2params_xam/","text":"Example and Test of effects2params} Function Documentation effects2params Example Source Code \"\"\" import numpy from curvefit.core.effects2params import effects2params # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- def identity_fun(x): return x num_fe = num_param num_x = (num_group + 1) * num_fe x = numpy.array(range(num_x), dtype=float) / num_x group_sizes = numpy.arange(num_group) * 2 + 1 num_obs = sum(group_sizes) covs = list() for k in range(num_param): covs.append(numpy.ones((num_obs, 1), dtype=float)) link_fun = [numpy.exp, identity_fun, numpy.exp] var_link_fun = num_param * [identity_fun] expand = False param = effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand ) # ---------------------------------------------------------------------- # check result eps99 = 99.0 * numpy.finfo(float).eps fe = x[0: num_fe] re = x[num_fe:].reshape((num_group, num_fe), order='C') fe_re = fe + re var = numpy.empty((num_group, num_fe), dtype=float) for j in range(num_fe): var[:, j] = var_link_fun[j](fe_re[:, j]) check = numpy.empty((num_param, num_group), dtype=float) for k in range(num_param): check[k, :] = link_fun[k](var[:, k] * covs[k][0]) # rel_error = param / check - 1.0 assert ((abs(rel_error) < eps99).all()) print('effects2params.py: OK')","title":"effects2params_xam"},{"location":"extract_md/effects2params_xam/#example-and-test-of-effects2params","text":"","title":"Example and Test of effects2params}"},{"location":"extract_md/effects2params_xam/#function-documentation","text":"effects2params","title":"Function Documentation"},{"location":"extract_md/effects2params_xam/#example-source-code","text":"\"\"\" import numpy from curvefit.core.effects2params import effects2params # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- def identity_fun(x): return x num_fe = num_param num_x = (num_group + 1) * num_fe x = numpy.array(range(num_x), dtype=float) / num_x group_sizes = numpy.arange(num_group) * 2 + 1 num_obs = sum(group_sizes) covs = list() for k in range(num_param): covs.append(numpy.ones((num_obs, 1), dtype=float)) link_fun = [numpy.exp, identity_fun, numpy.exp] var_link_fun = num_param * [identity_fun] expand = False param = effects2params( x, group_sizes, covs, link_fun, var_link_fun, expand ) # ---------------------------------------------------------------------- # check result eps99 = 99.0 * numpy.finfo(float).eps fe = x[0: num_fe] re = x[num_fe:].reshape((num_group, num_fe), order='C') fe_re = fe + re var = numpy.empty((num_group, num_fe), dtype=float) for j in range(num_fe): var[:, j] = var_link_fun[j](fe_re[:, j]) check = numpy.empty((num_param, num_group), dtype=float) for k in range(num_param): check[k, :] = link_fun[k](var[:, k] * covs[k][0]) # rel_error = param / check - 1.0 assert ((abs(rel_error) < eps99).all()) print('effects2params.py: OK')","title":"Example Source Code"},{"location":"extract_md/extract_md.py/","text":"Extracting Markdown Documentation From Source Code Syntax bin/extract_md.py extract_dir The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code. file_list The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from. extra_special_words The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below. Start Section The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot . mkdocs.yml For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:). Suspend Markdown It is possible do suspend the markdown output during a section. One begins the suspension with the command { suspend_markdown} and resumes the output with the command { resume_markdown} Note that this will also suspend the markdown processing; e.g., spell checking. Each suspend markdown must have a corresponding resume markdown in same section (between the corresponding begin markdown and end markdown commands). End Section The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section. Spell Checking Special words can be added to the correct spelling list for a particular section as follows: { spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included. Code Blocks A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes, for each code block, must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output. Indentation If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source. Wish List The following is a wish list for future improvements to extract_md.py : Testing Include an optional command line argument that indicates test mode and runs the extractor through some test files and makes sure the result is correct. Error Messaging Improve the error messaging so that it include the line number of the input file that the error occurred on. Source File Include the path to the source code file that the documentation was extracted from (probably at the end of the section). Double Word Errors Detect double word errors and allow for exceptions by specifying them in a double_word_markdown command. Moving Code Blocks Have a way to include code blocks that are not directly below and in the same file; e.g., one my automatically transfer the prototype for a function, in the same file or a different file, to the documentation for a section.","title":"extract_md.py"},{"location":"extract_md/extract_md.py/#extracting-markdown-documentation-from-source-code","text":"","title":"Extracting Markdown Documentation From Source Code"},{"location":"extract_md/extract_md.py/#syntax","text":"bin/extract_md.py","title":"Syntax"},{"location":"extract_md/extract_md.py/#extract_dir","text":"The variable extract_dir at top of bin/extract_md.py determines the sub-directory, below the docs directory, where the markdown files will be written. Any files names that end in .md in that directory will be removed at the beginning so that all the files in this directory have been extracted from the current version of the source code.","title":"extract_dir"},{"location":"extract_md/extract_md.py/#file_list","text":"The variable file_list at top of bin/extract_md.py is a list of file names, relative to the top git repository directory, that the markdown files will be extracted from.","title":"file_list"},{"location":"extract_md/extract_md.py/#extra_special_words","text":"The variable extra_special_words is a list of extra words that the spell checker will consider correct; see spell checking below.","title":"extra_special_words"},{"location":"extract_md/extract_md.py/#start-section","text":"The start of a markdown section of the input file is indicated by the following text: {begin_markdown section_name } Here section_name is the name of output file corresponding to this section. The possible characters in section_name are A-Z, a-z, 0-9, underbar _ , and dot .","title":"Start Section"},{"location":"extract_md/extract_md.py/#mkdocsyml","text":"For each section_name in the documentation there must be a line in the mkdocs.yml file fo the following form: - section_name : ' extract_dir / section_name .md' where there can be any number of spaces around the dash character (-) and the colon character (:).","title":"mkdocs.yml"},{"location":"extract_md/extract_md.py/#suspend-markdown","text":"It is possible do suspend the markdown output during a section. One begins the suspension with the command { suspend_markdown} and resumes the output with the command { resume_markdown} Note that this will also suspend the markdown processing; e.g., spell checking. Each suspend markdown must have a corresponding resume markdown in same section (between the corresponding begin markdown and end markdown commands).","title":"Suspend Markdown"},{"location":"extract_md/extract_md.py/#end-section","text":"The end of a markdown section of the input file is indicated by the following text: {end_markdown section_name } Here section_name must be the same as in the start of this markdown section.","title":"End Section"},{"location":"extract_md/extract_md.py/#spell-checking","text":"Special words can be added to the correct spelling list for a particular section as follows: { spell_markdown special_1 ... special_n } Here special_1 , ..., special_n are special words that are to be considered valid for this section. In the syntax above they are all on the same line, but they could be on different lines. Each word starts with an upper case letter, a lower case letter, or a back slash. The rest of the characters in a word are lower case letters. The case of the first letter does not matter when checking for special words; e.g., if abcd is special_1 then Abcd will be considered a valid word. The back slash is included at the beginning of a word so that latex commands are considered words. The latex commands corresponding to the letters in the greek alphabet are automatically included. Any latex commands in the extra_special_words are also automatically included.","title":"Spell Checking"},{"location":"extract_md/extract_md.py/#code-blocks","text":"A code block within a markdown section begins and ends with three back quotes. Thus there must be an even number of occurrences of three back quotes. The first three back quotes, for each code block, must have a language name directly after it. The language name must be a sequence of letters; e.g., python . The other characters on the same line as the three back quotes are not included in the markdown output. This enables one to begin or end a comment block without having those characters in the markdown output.","title":"Code Blocks"},{"location":"extract_md/extract_md.py/#indentation","text":"If all of the extracted markdown documentation for a section is indented by the same number of space characters, those space characters are not included in the markdown output. This enables one to indent the markdown so it is grouped with the proper code block in the source.","title":"Indentation"},{"location":"extract_md/extract_md.py/#wish-list","text":"The following is a wish list for future improvements to extract_md.py :","title":"Wish List"},{"location":"extract_md/extract_md.py/#testing","text":"Include an optional command line argument that indicates test mode and runs the extractor through some test files and makes sure the result is correct.","title":"Testing"},{"location":"extract_md/extract_md.py/#error-messaging","text":"Improve the error messaging so that it include the line number of the input file that the error occurred on.","title":"Error Messaging"},{"location":"extract_md/extract_md.py/#source-file","text":"Include the path to the source code file that the documentation was extracted from (probably at the end of the section).","title":"Source File"},{"location":"extract_md/extract_md.py/#double-word-errors","text":"Detect double word errors and allow for exceptions by specifying them in a double_word_markdown command.","title":"Double Word Errors"},{"location":"extract_md/extract_md.py/#moving-code-blocks","text":"Have a way to include code blocks that are not directly below and in the same file; e.g., one my automatically transfer the prototype for a function, in the same file or a different file, to the documentation for a section.","title":"Moving Code Blocks"},{"location":"extract_md/get_cppad_py.py/","text":"Download and Install cppad_py Syntax `bin/get_cppad_py.py [test] [debug] [prefix] Command Line Arguments The order of the command line arguments test , debug , and prefix does not matter test If this argument is present it must be test=True . In this case cppad_py will be tested before the install. debug If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed. prefix If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"get_cppad_py.py"},{"location":"extract_md/get_cppad_py.py/#download-and-install-cppad_py","text":"","title":"Download and Install cppad_py"},{"location":"extract_md/get_cppad_py.py/#syntax","text":"`bin/get_cppad_py.py [test] [debug] [prefix]","title":"Syntax"},{"location":"extract_md/get_cppad_py.py/#command-line-arguments","text":"The order of the command line arguments test , debug , and prefix does not matter","title":"Command Line Arguments"},{"location":"extract_md/get_cppad_py.py/#test","text":"If this argument is present it must be test=True . In this case cppad_py will be tested before the install.","title":"test"},{"location":"extract_md/get_cppad_py.py/#debug","text":"If this argument is present it must be debug=True . In this case the debug version of cppad_py will be built (possible tested) and installed.","title":"debug"},{"location":"extract_md/get_cppad_py.py/#prefix","text":"If this argument is present it must be prefix= path . In this case the path will be the prefix where cppad_py is installed. Otherwise the standard setup.py path is used.","title":"prefix"},{"location":"extract_md/get_started_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed Effects We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three. Random effects For this example the random effects are constrained to be zero. Covariates This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution Time Grid A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta . Measurement values We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise. Example Source Code # ------------------------------------------------------------------------- import pandas import numpy from curvefit.core.functions import expit , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 params_true = numpy . array ([ alpha_true , beta_true , p_true ]) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = expit ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data )] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = num_params * [ 'constant_one' ], col_group = 'data_group' , obs_space = expit , col_obs_se = 'measurement_std' ) alpha_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = numpy . log ( alpha_true ) / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) beta_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = beta_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) p_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = numpy . log ( p_true ) / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = numpy . exp , variables = [ alpha_intercept ]) beta = Parameter ( param_name = 'beta' , link_fun = lambda x : x , variables = [ beta_intercept ]) p = Parameter ( param_name = 'p' , link_fun = numpy . exp , variables = [ p_intercept ]) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } model = CoreModel ( param_set = parameters , curve_fun = expit , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = False ) # ------------------------------------------------------------------------- # check optimal parameters for i in range ( num_params ): rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol print ( 'get_started_old.py: OK' )","title":"get_started_xam"},{"location":"extract_md/get_started_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/get_started_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/get_started_xam/#fixed-effects","text":"We use the notation a , b and \\phi for the fixed effect corresponding to the parameters \\alpha , \\beta , p respectively. For this example, the link functions, that map from the fixed effects to the parameters, are \\begin{aligned} \\alpha & = \\exp( a ) \\\\ \\beta & = b \\\\ p & = \\exp( \\phi ) \\end{aligned} The fixed effects are initialized to be their true values divided by three.","title":"Fixed Effects"},{"location":"extract_md/get_started_xam/#random-effects","text":"For this example the random effects are constrained to be zero.","title":"Random effects"},{"location":"extract_md/get_started_xam/#covariates","text":"This example data set has two covariates, the constant one and a social distance measure. While the social distance is in the data set, it is not used.","title":"Covariates"},{"location":"extract_md/get_started_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/get_started_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: n_data = 21 # number simulated measurements to generate beta_true = 20.0 # max death rate at 20 days alpha_true = 2.0 / beta_true # alpha_true * beta_true = 2.0 p_true = 0.1 # maximum cumulative death fraction rel_tol = 1e-5 # relative tolerance used to check optimal solution","title":"Problem Settings"},{"location":"extract_md/get_started_xam/#time-grid","text":"A grid of n_data points in time, t_i , where t_i = \\beta_T / ( n_D - 1 ) where the subscript T denotes the true value of the corresponding parameter and n_D is the number of data points. The minimum value for this grid is zero and its maximum is \\beta .","title":"Time Grid"},{"location":"extract_md/get_started_xam/#measurement-values","text":"We simulate data, y_i , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_D - 1 y_i = f( t_i , \\alpha_T , \\beta_T , p_T ) Note that when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/get_started_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import pandas import numpy from curvefit.core.functions import expit , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # for this model number of parameters is same as number of fixed effects num_params = 3 num_fe = 3 params_true = numpy . array ([ alpha_true , beta_true , p_true ]) # ----------------------------------------------------------------------- # data_frame independent_var = numpy . array ( range ( n_data )) * beta_true / ( n_data - 1 ) measurement_value = expit ( independent_var , params_true ) measurement_std = n_data * [ 0.1 ] constant_one = n_data * [ 1.0 ] social_distance = [ 0.0 if i < n_data / 2 else 1.0 for i in range ( n_data )] data_group = n_data * [ 'world' ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'social_distance' : social_distance , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = num_params * [ 'constant_one' ], col_group = 'data_group' , obs_space = expit , col_obs_se = 'measurement_std' ) alpha_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = numpy . log ( alpha_true ) / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) beta_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = beta_true / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) p_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = numpy . log ( p_true ) / 3 , re_init = 0.0 , fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ 0.0 , 0.0 ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = numpy . exp , variables = [ alpha_intercept ]) beta = Parameter ( param_name = 'beta' , link_fun = lambda x : x , variables = [ beta_intercept ]) p = Parameter ( param_name = 'p' , link_fun = numpy . exp , variables = [ p_intercept ]) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'ftol' : 1e-12 , 'gtol' : 1e-12 , } model = CoreModel ( param_set = parameters , curve_fun = expit , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = False ) # ------------------------------------------------------------------------- # check optimal parameters for i in range ( num_params ): rel_error = params_estimate [ i ] / params_true [ i ] - 1.0 assert abs ( rel_error ) < rel_tol print ( 'get_started_old.py: OK' )","title":"Example Source Code"},{"location":"extract_md/local_deviations/","text":"curvefit.utils.smoothing.local_deviations Computes standard deviation within a neighborhood of covariate values Compute standard deviation of residuals within a neighborhood defined by col_axis . Optionally use the median absolute deviation as a robust estimator of the standard deviation. Arguments df (pd.DataFrame) : Residual data frame. col_val (str) : Name for column that store the residual. col_axis (List[str]) : List of two axis column names. axis_offset (List[int] | None, optional) : List of offset for each axis to make it suitable as numpy array. radius (List[int] | None, optional) : List of the neighbor radius for each dimension. robust (bool) : use the median absolute deviation * 1.4826 as a robust estimator for the standard deviation Returns pd.DataFrame : return the data frame an extra column for the median absolute deviation within a range","title":"local_deviations"},{"location":"extract_md/local_deviations/#curvefitutilssmoothinglocal_deviations","text":"","title":"curvefit.utils.smoothing.local_deviations"},{"location":"extract_md/local_deviations/#computes-standard-deviation-within-a-neighborhood-of-covariate-values","text":"Compute standard deviation of residuals within a neighborhood defined by col_axis . Optionally use the median absolute deviation as a robust estimator of the standard deviation.","title":"Computes standard deviation within a neighborhood of covariate values"},{"location":"extract_md/local_deviations/#arguments","text":"df (pd.DataFrame) : Residual data frame. col_val (str) : Name for column that store the residual. col_axis (List[str]) : List of two axis column names. axis_offset (List[int] | None, optional) : List of offset for each axis to make it suitable as numpy array. radius (List[int] | None, optional) : List of the neighbor radius for each dimension. robust (bool) : use the median absolute deviation * 1.4826 as a robust estimator for the standard deviation","title":"Arguments"},{"location":"extract_md/local_deviations/#returns","text":"pd.DataFrame : return the data frame an extra column for the median absolute deviation within a range","title":"Returns"},{"location":"extract_md/local_smoother/","text":"curvefit.utils.smoothing.local_smoother Runs a local smoother over a neighborhood of covariate values Runs a local smoother over a neighborhood defined by col_axis , where the neighborhood is defined by radius. Arguments df (pd.DataFrame) : data frame with values to smooth over col_val (str) : name for column that stores the value to smooth over col_axis (List[str]) : list of column names that store the axes corresponding to the variables that define the neighborhood radius (List[int] | None, optional) : list of the neighbor radius for each axis dimension Returns pd.DataFrame : return the data frame with an extra column for the smoothed value column called col_val + \"mean\"","title":"local_smoother"},{"location":"extract_md/local_smoother/#curvefitutilssmoothinglocal_smoother","text":"","title":"curvefit.utils.smoothing.local_smoother"},{"location":"extract_md/local_smoother/#runs-a-local-smoother-over-a-neighborhood-of-covariate-values","text":"Runs a local smoother over a neighborhood defined by col_axis , where the neighborhood is defined by radius.","title":"Runs a local smoother over a neighborhood of covariate values"},{"location":"extract_md/local_smoother/#arguments","text":"df (pd.DataFrame) : data frame with values to smooth over col_val (str) : name for column that stores the value to smooth over col_axis (List[str]) : list of column names that store the axes corresponding to the variables that define the neighborhood radius (List[int] | None, optional) : list of the neighbor radius for each axis dimension","title":"Arguments"},{"location":"extract_md/local_smoother/#returns","text":"pd.DataFrame : return the data frame with an extra column for the smoothed value column called col_val + \"mean\"","title":"Returns"},{"location":"extract_md/loss_xam/","text":"Example and Test of Loss Functions Function Documentation st_loss , normal_loss Example Source Code import sys import numpy import cppad_py import sandbox sandbox . path () import curvefit # ---------------------------------------------------------------------------- # Loss Functions # ---------------------------------------------------------------------------- eps99 = 99.0 * numpy . finfo ( float ) . eps # # test values for t, param r = numpy . array ( [ 1 , 2 , 3 ], dtype = float ) nu = numpy . array ( [ 3 , 2 , 1 ], dtype = float ) # ----------------------------------------------------------------------- # f(t) = st_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . st_loss ( ar , nu ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . st_loss ( r , nu ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- # f(t) = normal_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . normal_loss ( ar ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . normal_loss ( r ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- print ( 'loss.py: OK' ) sys . exit ( 0 )","title":"loss_xam"},{"location":"extract_md/loss_xam/#example-and-test-of-loss-functions","text":"","title":"Example and Test of Loss Functions"},{"location":"extract_md/loss_xam/#function-documentation","text":"st_loss , normal_loss","title":"Function Documentation"},{"location":"extract_md/loss_xam/#example-source-code","text":"import sys import numpy import cppad_py import sandbox sandbox . path () import curvefit # ---------------------------------------------------------------------------- # Loss Functions # ---------------------------------------------------------------------------- eps99 = 99.0 * numpy . finfo ( float ) . eps # # test values for t, param r = numpy . array ( [ 1 , 2 , 3 ], dtype = float ) nu = numpy . array ( [ 3 , 2 , 1 ], dtype = float ) # ----------------------------------------------------------------------- # f(t) = st_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . st_loss ( ar , nu ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . st_loss ( r , nu ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- # f(t) = normal_loss ar = cppad_py . independent ( r ) aloss = curvefit . core . functions . normal_loss ( ar ) ay = numpy . array ( [ aloss ] ) f = cppad_py . d_fun ( ar , ay ) # y = f . forward ( 0 , r ) check = curvefit . core . functions . normal_loss ( r ) rel_error = y [ 0 ] / check - 1.0 assert abs ( rel_error ) < eps99 # ----------------------------------------------------------------------- print ( 'loss.py: OK' ) sys . exit ( 0 )","title":"Example Source Code"},{"location":"extract_md/model_runner_xam/","text":"Example and Test of ModelRunner ModelRunner is the main class that connects all of the individual components of a curvefit model into a larger modeling pipeline. Here we will create examples of the building blocks that are necessary for a ModelRunner. The examples begin with an identical setup to the example in prior initializer . We will not use the prior initializer in this overall example, but it is an optional argument to the ModelRunner . Function Documentation Please see function documentation here: [ ModelRunner ][ModelRunner.md]. Importing Packages import numpy as np import pandas as pd from curvefit.core.functions import ln_gaussian_pdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import ParameterSet , Parameter , Variable from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt from curvefit.uncertainty.predictive_validity import PredictiveValidity from curvefit.uncertainty.residual_model import SmoothResidualModel from curvefit.uncertainty.draws import Draws from curvefit.run.model_run import ModelRunner Simulate Data Now we will set the simulation parameters. We will simulate 10 groups with 10 time points each, and their parameters are in the order alpha , beta , and p for the ln_gaussian_pdf functional form. The random effects variance will be used to simulate parameters from the mean alpha , beta , and p for each group. # simulation parameters np . random . seed ( 10 ) time = np . arange ( 20 ) n_groups = 10 fe_mean = np . array ([ 1. , 10. , 1. ]) re_var = np . array ([ 0.01 , 0.1 , 0.1 ]) # fe_gprior and re_gprior for the solver fe_gprior = [ 0. , np . inf ] re_gprior = [ 0. , np . inf ] # how much noise to add to the data data_noise = 1. Define a function to simulate data based on these parameters, and create simulated data. def simulate_data (): re_mean = np . random . normal ( fe_mean , re_var , size = ( n_groups , len ( fe_mean ))) group_names = [ f \"group_ { i } \" for i in range ( n_groups )] groups = [] for i in range ( n_groups ): truth = ln_gaussian_pdf ( time , re_mean [ i , :]) group_sim = truth + np . random . normal ( 0 , data_noise , size = len ( time )) groups . append ( pd . DataFrame ({ 'group_name' : group_names [ i ], 'obs' : group_sim , 'time' : time , 'truth' : truth })) group_data = pd . concat ( groups ) group_data [ 'intercept' ] = 1 return group_data df = simulate_data () # Use the `Data` object to store information about what the columns represent data = Data ( df = df , col_t = 'time' , col_obs = 'obs' , col_covs = [ 'intercept' ], obs_space = ln_gaussian_pdf , col_group = 'group_name' , obs_se_func = lambda x : 1 ) Create a Parameter Set We need to define variables, parameters that use those variables (in this case there are only intercepts -- and one covariate per parameter -- so the variables are effectively the same as the parameters), and a parameter set that collects all of that information into one object. alpha_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 0 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) beta_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 1 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) p_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 2 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) alpha = Parameter ( 'alpha' , link_fun = np . exp , variables = [ alpha_fe ]) beta = Parameter ( 'beta' , link_fun = lambda x : x , variables = [ beta_fe ]) p = Parameter ( 'p' , link_fun = lambda x : x , variables = [ p_fe ]) params = ParameterSet ([ alpha , beta , p ]) Models and Solvers We now need a Model that contains our parameter set and also which curve function we want to fit, and the loss function for the optimization. We also need to define a Solver that will actually perform the optimization. Finally, we will fit the solver to our simulated data. model = CoreModel ( param_set = params , curve_fun = ln_gaussian_pdf , loss_fun = normal_loss ) solver = ScipyOpt () Uncertainty For the uncertainty analysis, we need to specify three things: (1) a predictive validity object, (2) a residual model, and (3) a draws object. Predictive Validity The predictive validity object estimates out of sample residuals for every single data point in the dataset. For example, if you have 10 data points, the predictive validity object will start with the first data point in the time series, predict out the full time series, and then repeat by adding in the next time point and predicting out, etc. By the end, there is a square prediction matrix that can be converted to a residual matrix by subtracting off the observed data, for each group of size the number of observations for that group. The upper triangular indices (excluding the diagonal) are all of the out of sample residuals . Predictive validity provides a model agnostic way to get uncertainty in the predictions. To create a predictive validity object, you need to decide what space you want to evaluate the predictions. This space does not need to be the space that you're fitting the model in. pv = PredictiveValidity ( evaluation_space = ln_gaussian_pdf ) Residual Model The residual model is a predictive model for the residuals that are obtained from doing predictive validity. Right now there is only one type of residual model implemented. A SmoothResidualModel , with a description of the methods in the function documentation . The purpose is to understand how the coefficient of variation of the out of sample predictions from predictive validity changes with key factors. You need to pass bounds on the coefficient of variation of the predictions (can be [-np.inf, np.inf] to not pass bounds), a dictionary of covariates to use in predicting the residuals which for this model needs to be \"far_out\" and \"num_data\" . You also need a smoothing radius that defines a \"neighborhood\" based on those covariates, and how many times to run a local smoother over the residuals. rm = SmoothResidualModel ( cv_bounds = [ 1e-6 , np . inf ], covariates = { 'far_out' : None , 'num_data' : None }, num_smooth_iterations = 1 , smooth_radius = [ 2 , 2 ], robust = True ) Draws The predictive validity object and residual model come together to create uncertainty in the predictions in the Draws class. These are the final predictions. You can specify how many draws of the time series you want and for which time series: num_draws and prediction_times . draws = Draws ( num_draws = 1000 , prediction_times = np . arange ( 0. , 25. ) ) Model Runner Using all of the objects we've created so far we can instantiate a model runner. mr = ModelRunner ( data = data , model = model , solver = solver , residual_model = rm , predictive_validity = pv , draws = draws ) mr . run () We can now inspect the results for each group. # The residual matrix for group_9 from predictive validity analysis mr . predictive_validity . group_residuals [ 'group_9' ] . residual_matrix # The draws for group_9 mr . draws . get_draws ( 'group_9' ) # The mean estimates for group_9 (the first element is the mean) mr . draws . get_draws_summary ( 'group_9' )[ 0 ] # Example check that the mean estimate for this group are approximately the same as the truth for group in df . group_name . unique (): np . testing . assert_array_almost_equal ( df . loc [ df . group_name == group ] . truth . values , mr . draws . get_draws_summary ( group )[ 0 ][: 20 ], decimal = 0 ) print ( 'model_runner.py: OK' )","title":"model_runner_xam"},{"location":"extract_md/model_runner_xam/#example-and-test-of-modelrunner","text":"ModelRunner is the main class that connects all of the individual components of a curvefit model into a larger modeling pipeline. Here we will create examples of the building blocks that are necessary for a ModelRunner. The examples begin with an identical setup to the example in prior initializer . We will not use the prior initializer in this overall example, but it is an optional argument to the ModelRunner .","title":"Example and Test of ModelRunner"},{"location":"extract_md/model_runner_xam/#function-documentation","text":"Please see function documentation here: [ ModelRunner ][ModelRunner.md].","title":"Function Documentation"},{"location":"extract_md/model_runner_xam/#importing-packages","text":"import numpy as np import pandas as pd from curvefit.core.functions import ln_gaussian_pdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import ParameterSet , Parameter , Variable from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt from curvefit.uncertainty.predictive_validity import PredictiveValidity from curvefit.uncertainty.residual_model import SmoothResidualModel from curvefit.uncertainty.draws import Draws from curvefit.run.model_run import ModelRunner","title":"Importing Packages"},{"location":"extract_md/model_runner_xam/#simulate-data","text":"Now we will set the simulation parameters. We will simulate 10 groups with 10 time points each, and their parameters are in the order alpha , beta , and p for the ln_gaussian_pdf functional form. The random effects variance will be used to simulate parameters from the mean alpha , beta , and p for each group. # simulation parameters np . random . seed ( 10 ) time = np . arange ( 20 ) n_groups = 10 fe_mean = np . array ([ 1. , 10. , 1. ]) re_var = np . array ([ 0.01 , 0.1 , 0.1 ]) # fe_gprior and re_gprior for the solver fe_gprior = [ 0. , np . inf ] re_gprior = [ 0. , np . inf ] # how much noise to add to the data data_noise = 1. Define a function to simulate data based on these parameters, and create simulated data. def simulate_data (): re_mean = np . random . normal ( fe_mean , re_var , size = ( n_groups , len ( fe_mean ))) group_names = [ f \"group_ { i } \" for i in range ( n_groups )] groups = [] for i in range ( n_groups ): truth = ln_gaussian_pdf ( time , re_mean [ i , :]) group_sim = truth + np . random . normal ( 0 , data_noise , size = len ( time )) groups . append ( pd . DataFrame ({ 'group_name' : group_names [ i ], 'obs' : group_sim , 'time' : time , 'truth' : truth })) group_data = pd . concat ( groups ) group_data [ 'intercept' ] = 1 return group_data df = simulate_data () # Use the `Data` object to store information about what the columns represent data = Data ( df = df , col_t = 'time' , col_obs = 'obs' , col_covs = [ 'intercept' ], obs_space = ln_gaussian_pdf , col_group = 'group_name' , obs_se_func = lambda x : 1 )","title":"Simulate Data"},{"location":"extract_md/model_runner_xam/#create-a-parameter-set","text":"We need to define variables, parameters that use those variables (in this case there are only intercepts -- and one covariate per parameter -- so the variables are effectively the same as the parameters), and a parameter set that collects all of that information into one object. alpha_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 0 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) beta_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 1 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) p_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 2 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) alpha = Parameter ( 'alpha' , link_fun = np . exp , variables = [ alpha_fe ]) beta = Parameter ( 'beta' , link_fun = lambda x : x , variables = [ beta_fe ]) p = Parameter ( 'p' , link_fun = lambda x : x , variables = [ p_fe ]) params = ParameterSet ([ alpha , beta , p ])","title":"Create a Parameter Set"},{"location":"extract_md/model_runner_xam/#models-and-solvers","text":"We now need a Model that contains our parameter set and also which curve function we want to fit, and the loss function for the optimization. We also need to define a Solver that will actually perform the optimization. Finally, we will fit the solver to our simulated data. model = CoreModel ( param_set = params , curve_fun = ln_gaussian_pdf , loss_fun = normal_loss ) solver = ScipyOpt ()","title":"Models and Solvers"},{"location":"extract_md/model_runner_xam/#uncertainty","text":"For the uncertainty analysis, we need to specify three things: (1) a predictive validity object, (2) a residual model, and (3) a draws object.","title":"Uncertainty"},{"location":"extract_md/model_runner_xam/#predictive-validity","text":"The predictive validity object estimates out of sample residuals for every single data point in the dataset. For example, if you have 10 data points, the predictive validity object will start with the first data point in the time series, predict out the full time series, and then repeat by adding in the next time point and predicting out, etc. By the end, there is a square prediction matrix that can be converted to a residual matrix by subtracting off the observed data, for each group of size the number of observations for that group. The upper triangular indices (excluding the diagonal) are all of the out of sample residuals . Predictive validity provides a model agnostic way to get uncertainty in the predictions. To create a predictive validity object, you need to decide what space you want to evaluate the predictions. This space does not need to be the space that you're fitting the model in. pv = PredictiveValidity ( evaluation_space = ln_gaussian_pdf )","title":"Predictive Validity"},{"location":"extract_md/model_runner_xam/#residual-model","text":"The residual model is a predictive model for the residuals that are obtained from doing predictive validity. Right now there is only one type of residual model implemented. A SmoothResidualModel , with a description of the methods in the function documentation . The purpose is to understand how the coefficient of variation of the out of sample predictions from predictive validity changes with key factors. You need to pass bounds on the coefficient of variation of the predictions (can be [-np.inf, np.inf] to not pass bounds), a dictionary of covariates to use in predicting the residuals which for this model needs to be \"far_out\" and \"num_data\" . You also need a smoothing radius that defines a \"neighborhood\" based on those covariates, and how many times to run a local smoother over the residuals. rm = SmoothResidualModel ( cv_bounds = [ 1e-6 , np . inf ], covariates = { 'far_out' : None , 'num_data' : None }, num_smooth_iterations = 1 , smooth_radius = [ 2 , 2 ], robust = True )","title":"Residual Model"},{"location":"extract_md/model_runner_xam/#draws","text":"The predictive validity object and residual model come together to create uncertainty in the predictions in the Draws class. These are the final predictions. You can specify how many draws of the time series you want and for which time series: num_draws and prediction_times . draws = Draws ( num_draws = 1000 , prediction_times = np . arange ( 0. , 25. ) )","title":"Draws"},{"location":"extract_md/model_runner_xam/#model-runner","text":"Using all of the objects we've created so far we can instantiate a model runner. mr = ModelRunner ( data = data , model = model , solver = solver , residual_model = rm , predictive_validity = pv , draws = draws ) mr . run () We can now inspect the results for each group. # The residual matrix for group_9 from predictive validity analysis mr . predictive_validity . group_residuals [ 'group_9' ] . residual_matrix # The draws for group_9 mr . draws . get_draws ( 'group_9' ) # The mean estimates for group_9 (the first element is the mean) mr . draws . get_draws_summary ( 'group_9' )[ 0 ] # Example check that the mean estimate for this group are approximately the same as the truth for group in df . group_name . unique (): np . testing . assert_array_almost_equal ( df . loc [ df . group_name == group ] . truth . values , mr . draws . get_draws_summary ( group )[ 0 ][: 20 ], decimal = 0 ) print ( 'model_runner.py: OK' )","title":"Model Runner"},{"location":"extract_md/normal_loss/","text":"Gaussian Loss Function Syntax loss = curvefit.core.loss_fun.normal_loss(r) r is a numpy vector of normalized residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values. Distribution The Gaussian distribution is f(x) = \\exp \\left[ - (1/2) ( x - \\mu )^2 / \\sigma^2 \\right] / \\left( \\sigma \\sqrt{ 2 \\pi } \\right) where \\mu is the mean and \\sigma is the standard deviation. Negative log Taking the negative log of the distribution function we get - \\log [ f(x) ] = (1/2) ( x - \\mu )^2 / \\sigma^2 + c where c is constant w.r.t. x . loss The return value loss is a scalar equal to \\frac{1}{2} \\sum_{i=1}^{n-1} r_i^2 where r_i = ( x_i - \\mu)) / \\sigma . Example loss_xam","title":"normal_loss"},{"location":"extract_md/normal_loss/#gaussian-loss-function","text":"","title":"Gaussian Loss Function"},{"location":"extract_md/normal_loss/#syntax","text":"loss = curvefit.core.loss_fun.normal_loss(r)","title":"Syntax"},{"location":"extract_md/normal_loss/#r","text":"is a numpy vector of normalized residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values.","title":"r"},{"location":"extract_md/normal_loss/#distribution","text":"The Gaussian distribution is f(x) = \\exp \\left[ - (1/2) ( x - \\mu )^2 / \\sigma^2 \\right] / \\left( \\sigma \\sqrt{ 2 \\pi } \\right) where \\mu is the mean and \\sigma is the standard deviation.","title":"Distribution"},{"location":"extract_md/normal_loss/#negative-log","text":"Taking the negative log of the distribution function we get - \\log [ f(x) ] = (1/2) ( x - \\mu )^2 / \\sigma^2 + c where c is constant w.r.t. x .","title":"Negative log"},{"location":"extract_md/normal_loss/#loss","text":"The return value loss is a scalar equal to \\frac{1}{2} \\sum_{i=1}^{n-1} r_i^2 where r_i = ( x_i - \\mu)) / \\sigma .","title":"loss"},{"location":"extract_md/normal_loss/#example","text":"loss_xam","title":"Example"},{"location":"extract_md/objective_fun/","text":"curvefit.core.objective_fun.objective_fun Curve Fitting Objective Function of Fixed and Random Effects Syntax obj_val = curvefit . core . objective_fun . objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior , re_zero_sum_std ) Notation num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = fe_gprior.shape[0] is the number of fixed effects. num_group = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs numpy array containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array . Arguments x (np.array) : is a one dimension numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. t (np.array) : is a one dimension numpy array with length num_obs containing the value of the independent variable corresponding to each observation. obs (np.array) : is a one dimension numpy array with length num_obs containing the observations (i.e. measurements). obs_se (np.array) : is a one dimension numpy array with length num_obs containing the standard deviation for the corresponding observation. covs (List[np.ndarray]) : is a list with length equal to the number of parameters and covs[k] is a two dimension numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. group_sizes (List[int]) : the observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs . model_fun (Callable) : this vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The observation residual vector has length num_obs and is given by obs_res = ( obs - model_fun ( t , params )) / obs_se loss_fun (Callable) : this scalar value function maps the observation residual vector to the corresponding contribution to the objective function. For example, if loss_fun corresponds to a Gaussian likelihood, it is equal to gaussian_loss ( obs_res ) = 0.5 * sum ( obs_res * obs_res ) link_fun (List[callable]) : the parameter link functions, see Parameter var_fun (List[callable]) : the variable link functions, see Variable fe_gprior (np.array) : is an num_fe by two numpy array. The value fe_gprior[j][0] is the prior mean for the j-th fixed effect and fe_gprior[j][1] is its standard deviation. If fe is the fixed effect sub-vector of x , the prior residual for the fixed effects is fe_res = ( fe . T - fe_gprior [:, 0 ] ) / fe_gprior [:, 1 ] where `fe.T` denotes the transpose of `fe` . re_gprior (np.array) : is an num_fe by num_groups by by two numpy array, re_gprior[j,i,0] ( re_gprior[j,i,1] ) is the mean (standard deviation) for the random effect corresponding to the j-th fixed effect and the i-th group. If re is the matrix of random effect corresponding to x , the prior residual for the random effects is re_res = ( re . T - re_gprior [:,:, 0 ] ) / re_gprior [:,:, 1 ] param_gprior (Tuple[Callable, Tuple[int, int]]) : is a list with two elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) There is a subtlety here, column dimension of the *params* above is *num_groups* (not *num_obs). The value `param_gprior[1][0]` ( `param_gprior[1][1]` ) is a numpy array corresponding to the mean (standard deviation) for *range_gprior*. The prior residual for the parameters is param_res = ( range_gprior - param_gprior [ 1 ][[ 0 ]]) / param_gprior [ 1 ][ 1 ] re_zero_sum_std (np.array) : is a vector with length num_fe , re_zero_sum_std[j] is the standard deviation for the sum of the random effect corresponding to the j-th fixed effect. Note that a standard deviation of np.inf corresponds to no prior on the sum of the corresponding random effects. Returns obj_val The return val is a float equal to the objective function obj_val = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) + gaussian_loss ( re_res ) + gaussian_loss ( param_res ) Example objective_fun_xam","title":"objective_fun"},{"location":"extract_md/objective_fun/#curvefitcoreobjective_funobjective_fun","text":"","title":"curvefit.core.objective_fun.objective_fun"},{"location":"extract_md/objective_fun/#curve-fitting-objective-function-of-fixed-and-random-effects","text":"","title":"Curve Fitting Objective Function of Fixed and Random Effects"},{"location":"extract_md/objective_fun/#syntax","text":"obj_val = curvefit . core . objective_fun . objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior , re_zero_sum_std )","title":"Syntax"},{"location":"extract_md/objective_fun/#notation","text":"num_obs = len(obs) is the number of observations (measurements) num_param = len(covs) is the number of parameters in the model. num_fe = fe_gprior.shape[0] is the number of fixed effects. num_group = len(group_sizes) is the number of groups params = `effects2params(x, group_sizes, covs, link_fun, var_link_fun) is a num_param by num_obs numpy array containing the parameters corresponding to each observation; see effects2params . A vector is either a list or a one dimension numpy.array .","title":"Notation"},{"location":"extract_md/objective_fun/#arguments","text":"x (np.array) : is a one dimension numpy array contain a value for the fixed effects followed by the random effects. The random effects are divided into sub-vectors with length equal to the number of fixed effects. The i-th sub-vector corresponds to the i-th group of observations. t (np.array) : is a one dimension numpy array with length num_obs containing the value of the independent variable corresponding to each observation. obs (np.array) : is a one dimension numpy array with length num_obs containing the observations (i.e. measurements). obs_se (np.array) : is a one dimension numpy array with length num_obs containing the standard deviation for the corresponding observation. covs (List[np.ndarray]) : is a list with length equal to the number of parameters and covs[k] is a two dimension numpy array with the following contents: -- covs[k].shape[0] is the number of observations -- covs[k].shape[1] is the number of fixed effects corresponding to the k-th parameter. -- covs[k][i, ell] is the covariate value corresponding to the i-th observation and ell-th covariate for the k-th parameter. group_sizes (List[int]) : the observations are divided into groups. The first group_sizes[0] observations correspond to the first group, the next group_sizes[1] corresponds to the section group, and so on. The the sum of the group sizes is equal to num_obs . model_fun (Callable) : this vector valued function vector values maps parameter values, params returned by effects2params , to the model for the corresponding noiseless observations. The observation residual vector has length num_obs and is given by obs_res = ( obs - model_fun ( t , params )) / obs_se loss_fun (Callable) : this scalar value function maps the observation residual vector to the corresponding contribution to the objective function. For example, if loss_fun corresponds to a Gaussian likelihood, it is equal to gaussian_loss ( obs_res ) = 0.5 * sum ( obs_res * obs_res ) link_fun (List[callable]) : the parameter link functions, see Parameter var_fun (List[callable]) : the variable link functions, see Variable fe_gprior (np.array) : is an num_fe by two numpy array. The value fe_gprior[j][0] is the prior mean for the j-th fixed effect and fe_gprior[j][1] is its standard deviation. If fe is the fixed effect sub-vector of x , the prior residual for the fixed effects is fe_res = ( fe . T - fe_gprior [:, 0 ] ) / fe_gprior [:, 1 ] where `fe.T` denotes the transpose of `fe` . re_gprior (np.array) : is an num_fe by num_groups by by two numpy array, re_gprior[j,i,0] ( re_gprior[j,i,1] ) is the mean (standard deviation) for the random effect corresponding to the j-th fixed effect and the i-th group. If re is the matrix of random effect corresponding to x , the prior residual for the random effects is re_res = ( re . T - re_gprior [:,:, 0 ] ) / re_gprior [:,:, 1 ] param_gprior (Tuple[Callable, Tuple[int, int]]) : is a list with two elements. The first element is a function of the params and its result is a numpy array. We use the notation range_gprior = param_gprior [ 0 ]( params ) There is a subtlety here, column dimension of the *params* above is *num_groups* (not *num_obs). The value `param_gprior[1][0]` ( `param_gprior[1][1]` ) is a numpy array corresponding to the mean (standard deviation) for *range_gprior*. The prior residual for the parameters is param_res = ( range_gprior - param_gprior [ 1 ][[ 0 ]]) / param_gprior [ 1 ][ 1 ] re_zero_sum_std (np.array) : is a vector with length num_fe , re_zero_sum_std[j] is the standard deviation for the sum of the random effect corresponding to the j-th fixed effect. Note that a standard deviation of np.inf corresponds to no prior on the sum of the corresponding random effects.","title":"Arguments"},{"location":"extract_md/objective_fun/#returns","text":"","title":"Returns"},{"location":"extract_md/objective_fun/#obj_val","text":"The return val is a float equal to the objective function obj_val = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) + gaussian_loss ( re_res ) + gaussian_loss ( param_res )","title":"obj_val"},{"location":"extract_md/objective_fun/#example","text":"objective_fun_xam","title":"Example"},{"location":"extract_md/objective_fun_xam/","text":"Example and Test of objective_fun Function Documentation [objective_fun][objective_fun.md] Example Source Code import sys import numpy from curvefit.core.functions import gaussian_cdf from curvefit.core.objective_fun import objective_fun from curvefit.core.effects2params import effects2params , unzip_x # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- # arguments to objective_fun def identity_fun ( x ): return x def gaussian_loss ( x ): return numpy . sum ( x * x ) / 2.0 # set parameters for the objective function num_fe = num_param num_re = num_group * num_fe fe = numpy . array ( range ( num_fe ), dtype = float ) / num_fe re = numpy . array ( range ( num_re ), dtype = float ) / num_re group_sizes = ( numpy . arange ( num_group ) + 1 ) * 2 # observations x = numpy . concatenate (( fe , re )) num_obs = sum ( group_sizes ) t = numpy . arange ( num_obs , dtype = float ) obs = numpy . arange ( num_obs , dtype = float ) / num_obs obs_se = ( obs + 1.0 ) / 10.0 # covariates covs = list () for k in range ( num_param ): covs . append ( numpy . ones (( num_obs , 1 ), dtype = float )) # fit, loss and link functions model_fun = gaussian_cdf loss_fun = gaussian_loss link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] # fixed effects Gaussian prior fe_gprior = numpy . empty (( num_fe , 2 ), dtype = float ) for j in range ( num_fe ): fe_gprior [ j , 0 ] = j / ( 2.0 * num_fe ) fe_gprior [:, 1 ] = 1.0 + fe_gprior [:, 0 ] * 1.2 # random effects Gaussian prior re_gprior = numpy . empty (( num_fe , num_group , 2 ), dtype = float ) # parameter functional Gaussian prior param_gprior_mean = numpy . empty (( num_param , num_group ), dtype = float ) for i in range ( num_group ): for j in range ( num_fe ): # the matrix re_gprior[:,:,0] is the transposed from the order in re re_gprior [ j , i , 0 ] = ( i + j ) / ( 2.0 * ( num_fe + num_re )) k = j param_gprior_mean [ k , i ] = ( i + k ) / ( 3.0 * ( num_fe + num_re )) re_gprior [:, :, 1 ] = ( 1.0 + re_gprior [:, :, 0 ] / 3.0 ) param_gprior_std = ( 1.0 + param_gprior_mean / 2.0 ) param_gprior_fun = identity_fun param_gprior = [ param_gprior_fun , param_gprior_mean , param_gprior_std ] re_zero_sum_std = numpy . array ( num_fe * [ numpy . inf ] ) # ----------------------------------------------------------------------- # call to objective_fun obj_val = objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior , re_zero_sum_std , ) # ----------------------------------------------------------------------- # check objective_fun return value expand = True param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) fe , re = unzip_x ( x , num_group , num_fe ) obs_res = ( obs - model_fun ( t , param )) / obs_se fe_res = ( fe . T - fe_gprior [:, 0 ]) / fe_gprior [:, 1 ] re_res = ( re . T - re_gprior [:, :, 0 ]) / re_gprior [:, :, 1 ] expand = False param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) range_gprior = param_gprior [ 0 ]( param ) param_res = ( range_gprior - param_gprior [ 1 ][ 0 ]) / param_gprior [ 1 ][ 1 ] check = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) check += gaussian_loss ( re_res ) + gaussian_loss ( param_res ) rel_error = obj_val / check - 1.0 eps99 = 99.0 * numpy . finfo ( float ) . eps assert abs ( rel_error ) < eps99 print ( 'objective_fun.py: OK' )","title":"objective_fun_xam"},{"location":"extract_md/objective_fun_xam/#example-and-test-of-objective_fun","text":"","title":"Example and Test of objective_fun"},{"location":"extract_md/objective_fun_xam/#function-documentation","text":"[objective_fun][objective_fun.md]","title":"Function Documentation"},{"location":"extract_md/objective_fun_xam/#example-source-code","text":"import sys import numpy from curvefit.core.functions import gaussian_cdf from curvefit.core.objective_fun import objective_fun from curvefit.core.effects2params import effects2params , unzip_x # ----------------------------------------------------------------------- # Test parameters num_param = 3 num_group = 2 # ----------------------------------------------------------------------- # arguments to objective_fun def identity_fun ( x ): return x def gaussian_loss ( x ): return numpy . sum ( x * x ) / 2.0 # set parameters for the objective function num_fe = num_param num_re = num_group * num_fe fe = numpy . array ( range ( num_fe ), dtype = float ) / num_fe re = numpy . array ( range ( num_re ), dtype = float ) / num_re group_sizes = ( numpy . arange ( num_group ) + 1 ) * 2 # observations x = numpy . concatenate (( fe , re )) num_obs = sum ( group_sizes ) t = numpy . arange ( num_obs , dtype = float ) obs = numpy . arange ( num_obs , dtype = float ) / num_obs obs_se = ( obs + 1.0 ) / 10.0 # covariates covs = list () for k in range ( num_param ): covs . append ( numpy . ones (( num_obs , 1 ), dtype = float )) # fit, loss and link functions model_fun = gaussian_cdf loss_fun = gaussian_loss link_fun = [ numpy . exp , identity_fun , numpy . exp ] var_link_fun = num_param * [ identity_fun ] # fixed effects Gaussian prior fe_gprior = numpy . empty (( num_fe , 2 ), dtype = float ) for j in range ( num_fe ): fe_gprior [ j , 0 ] = j / ( 2.0 * num_fe ) fe_gprior [:, 1 ] = 1.0 + fe_gprior [:, 0 ] * 1.2 # random effects Gaussian prior re_gprior = numpy . empty (( num_fe , num_group , 2 ), dtype = float ) # parameter functional Gaussian prior param_gprior_mean = numpy . empty (( num_param , num_group ), dtype = float ) for i in range ( num_group ): for j in range ( num_fe ): # the matrix re_gprior[:,:,0] is the transposed from the order in re re_gprior [ j , i , 0 ] = ( i + j ) / ( 2.0 * ( num_fe + num_re )) k = j param_gprior_mean [ k , i ] = ( i + k ) / ( 3.0 * ( num_fe + num_re )) re_gprior [:, :, 1 ] = ( 1.0 + re_gprior [:, :, 0 ] / 3.0 ) param_gprior_std = ( 1.0 + param_gprior_mean / 2.0 ) param_gprior_fun = identity_fun param_gprior = [ param_gprior_fun , param_gprior_mean , param_gprior_std ] re_zero_sum_std = numpy . array ( num_fe * [ numpy . inf ] ) # ----------------------------------------------------------------------- # call to objective_fun obj_val = objective_fun ( x , t , obs , obs_se , covs , group_sizes , model_fun , loss_fun , link_fun , var_link_fun , fe_gprior , re_gprior , param_gprior , re_zero_sum_std , ) # ----------------------------------------------------------------------- # check objective_fun return value expand = True param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) fe , re = unzip_x ( x , num_group , num_fe ) obs_res = ( obs - model_fun ( t , param )) / obs_se fe_res = ( fe . T - fe_gprior [:, 0 ]) / fe_gprior [:, 1 ] re_res = ( re . T - re_gprior [:, :, 0 ]) / re_gprior [:, :, 1 ] expand = False param = effects2params ( x , group_sizes , covs , link_fun , var_link_fun , expand ) range_gprior = param_gprior [ 0 ]( param ) param_res = ( range_gprior - param_gprior [ 1 ][ 0 ]) / param_gprior [ 1 ][ 1 ] check = loss_fun ( obs_res ) + gaussian_loss ( fe_res ) check += gaussian_loss ( re_res ) + gaussian_loss ( param_res ) rel_error = obj_val / check - 1.0 eps99 = 99.0 * numpy . finfo ( float ) . eps assert abs ( rel_error ) < eps99 print ( 'objective_fun.py: OK' )","title":"Example Source Code"},{"location":"extract_md/param_time_fun/","text":"Predefined Parametric Functions of Time Syntax result = curvefit . core . functions . fun ( t , params ) Arguments t (np.array) : This is a list or one dimensional numpy.array . params (np.array | List[float]) : This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2] - fun (Callable) : the possible values for fun are listed in the subheadings below: expit This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] } ln_expit This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p ) gaussian_cdf This is the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right] ln_gaussian_cdf This is the log of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) gaussian_pdf This is the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) ln_gaussian_pdf This is the log of the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) dgaussian_pdf This is the second derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) Result The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ]) Example param_time_fun_xam","title":"param_time_fun"},{"location":"extract_md/param_time_fun/#predefined-parametric-functions-of-time","text":"","title":"Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun/#syntax","text":"result = curvefit . core . functions . fun ( t , params )","title":"Syntax"},{"location":"extract_md/param_time_fun/#arguments","text":"t (np.array) : This is a list or one dimensional numpy.array . params (np.array | List[float]) : This is either a list , or numpy.array with one or two dimensions. In any case, len(params) == 3 . If params is a two dimensional array, params.shape[1] == len(t) . We use the notation below for the values in params : Notation Definition \\alpha params[0] \\beta params[1] p params[2] - fun (Callable) : the possible values for fun are listed in the subheadings below:","title":"Arguments"},{"location":"extract_md/param_time_fun/#expit","text":"This is the generalized logistic function which is defined by \\mbox{expit} ( t , \\alpha , \\beta , p ) = \\frac{p}{ 1.0 + \\exp [ - \\alpha ( t - \\beta ) ] }","title":"expit"},{"location":"extract_md/param_time_fun/#ln_expit","text":"This is the log of the generalized logistic function which is defined by \\mbox{ln_expit} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{expit} ( t , \\alpha , \\beta , p )","title":"ln_expit"},{"location":"extract_md/param_time_fun/#gaussian_cdf","text":"This is the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\frac{p}{2} \\left[ 1.0 + \\frac{2}{\\pi} \\int_0^{\\alpha(t-\\beta)} \\exp ( - \\tau^2 ) d \\tau \\right]","title":"gaussian_cdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_cdf","text":"This is the log of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_cdf"},{"location":"extract_md/param_time_fun/#gaussian_pdf","text":"This is the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_cdf} ( t , \\alpha , \\beta , p )","title":"gaussian_pdf"},{"location":"extract_md/param_time_fun/#ln_gaussian_pdf","text":"This is the log of the derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{ln_gaussian_cdf} ( t , \\alpha , \\beta , p ) = \\log \\circ \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"ln_gaussian_pdf"},{"location":"extract_md/param_time_fun/#dgaussian_pdf","text":"This is the second derivative of the generalized Gaussian cumulative distribution function which is defined by \\mbox{dgaussian_pdf} ( t , \\alpha , \\beta , p ) = \\partial_t \\; \\mbox{gaussian_pdf} ( t , \\alpha , \\beta , p )","title":"dgaussian_pdf"},{"location":"extract_md/param_time_fun/#result","text":"The result is a list or one dimensional numpy.array with len(result) == len(t) . If params is a list or one dimensional array result [ i ] = fun ( t [ i ], alpha , beta , p ) If params is a two dimensional array result [ i ] = fun ( t [ i ], alpha [ i ], beta [ i ], p [ i ])","title":"Result"},{"location":"extract_md/param_time_fun/#example","text":"param_time_fun_xam","title":"Example"},{"location":"extract_md/param_time_fun_xam/","text":"Example and Test of Predefined Parametric Functions of Time Function Documentation param_time_fun Example Source Code import numpy import scipy from curvefit.core.functions import * eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 def eval_expit ( t , alpha , beta , p ): return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ))) def eval_gaussian_cdf ( t , alpha , beta , p ): z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z )) / 2.0 # test values for t, alpha, beta, p t = numpy . array ([ 5.0 , 10.0 ]) beta = numpy . array ([ 30.0 , 20.0 ]) alpha = 2.0 / beta p = numpy . array ([ 0.1 , 0.2 ]) params = numpy . vstack (( alpha , beta , p )) # check gaussian_cdf value = gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # check ln_gaussian_cdf value = ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # check gaussian_pdf step = sqrt_eps * beta value = gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # check ln_gaussian_pdf value = ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # check_dgaussian_pdf step = quad_eps * beta value = dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) print ( 'param_time_fun.py: OK' )","title":"param_time_fun_xam"},{"location":"extract_md/param_time_fun_xam/#example-and-test-of-predefined-parametric-functions-of-time","text":"","title":"Example and Test of Predefined Parametric Functions of Time"},{"location":"extract_md/param_time_fun_xam/#function-documentation","text":"param_time_fun","title":"Function Documentation"},{"location":"extract_md/param_time_fun_xam/#example-source-code","text":"import numpy import scipy from curvefit.core.functions import * eps99 = 99.0 * numpy . finfo ( float ) . eps sqrt_eps = numpy . sqrt ( numpy . finfo ( float ) . eps ) quad_eps = numpy . sqrt ( sqrt_eps ) d_tolerance = 1e-6 def eval_expit ( t , alpha , beta , p ): return p / ( 1.0 + numpy . exp ( - alpha * ( t - beta ))) def eval_gaussian_cdf ( t , alpha , beta , p ): z = alpha * ( t - beta ) return p * ( 1.0 + scipy . special . erf ( z )) / 2.0 # test values for t, alpha, beta, p t = numpy . array ([ 5.0 , 10.0 ]) beta = numpy . array ([ 30.0 , 20.0 ]) alpha = 2.0 / beta p = numpy . array ([ 0.1 , 0.2 ]) params = numpy . vstack (( alpha , beta , p )) # check gaussian_cdf value = gaussian_cdf ( t , params ) check = eval_gaussian_cdf ( t , alpha , beta , p ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # check ln_gaussian_cdf value = ln_gaussian_cdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < eps99 ) # check gaussian_pdf step = sqrt_eps * beta value = gaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - check_m ) / ( 2.0 * step ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # check ln_gaussian_pdf value = ln_gaussian_pdf ( t , params ) check = numpy . log ( check ) rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) # check_dgaussian_pdf step = quad_eps * beta value = dgaussian_pdf ( t , params ) check_m = eval_gaussian_cdf ( t - step , alpha , beta , p ) check_0 = eval_gaussian_cdf ( t , alpha , beta , p ) check_p = eval_gaussian_cdf ( t + step , alpha , beta , p ) check = ( check_p - 2.0 * check_0 + check_m ) / step ** 2 rel_error = value / check - 1.0 assert all ( abs ( rel_error ) < d_tolerance ) print ( 'param_time_fun.py: OK' )","title":"Example Source Code"},{"location":"extract_md/prior_initializer_xam/","text":"Example of Prior Initializer Function Documentation See PriorInitializer for function documentation. Example In this example, we will simulate data from the curvefit.core.functions.ln_gaussian_pdf with n_groups different groups random effects, and fit a PriorInitializer to these groups to get a new ParameterSet to be used for fitting other groups. The idea is that if you have more complete time series' for some groups, then you could first fit a model(s) to these groups alone, extract information about fixed effects means, random effects variances, etc., and then use that information to set better priors for the groups that have less complete time series'. This example shows how to generate a new ParameterSet using PriorInitializer that could then be used to fit other models. First, we need to import all of the packages and modules that we will use: import numpy as np import pandas as pd from curvefit.core.functions import ln_gaussian_pdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import ParameterSet , Parameter , Variable from curvefit.initializer.initializer import PriorInitializer from curvefit.initializer.initializer_component import BetaPrior from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt Simulate Data Now we will set the simulation parameters. We will simulate 10 groups with 10 time points each, and their parameters are in the order alpha , beta , and p for the ln_gaussian_pdf functional form. The random effects variance will be used to simulate parameters from the mean alpha , beta , and p for each group. # simulation parameters np . random . seed ( 10 ) time = np . arange ( 10 ) n_groups = 10 fe_mean = np . array ([ 1 , 3. , 1. ]) re_var = np . array ([ 0.01 , 0.1 , 0.1 ]) # fe_gprior and re_gprior for the solver fe_gprior = [ 0. , np . inf ] re_gprior = [ 0. , np . inf ] # how much noise to add to the data data_noise = 1. Define a function to simulate data based on these parameters, and create simulated data. def simulate_data (): re_mean = np . random . normal ( fe_mean , re_var , size = ( n_groups , len ( fe_mean ))) group_names = [ f \"group_ { i } \" for i in range ( n_groups )] groups = [] for i in range ( n_groups ): group_sim = ln_gaussian_pdf ( time , re_mean [ i , :]) + np . random . normal ( 0 , data_noise , size = len ( time )) groups . append ( pd . DataFrame ({ 'group_name' : group_names [ i ], 'obs' : group_sim , 'time' : time })) group_data = pd . concat ( groups ) group_data [ 'intercept' ] = 1 return group_data df = simulate_data () # Use the `Data` object to store information about what the columns represent data = Data ( df = df , col_t = 'time' , col_obs = 'obs' , col_covs = [ 'intercept' ], obs_space = ln_gaussian_pdf , col_group = 'group_name' , obs_se_func = lambda x : 1 ) Create a Parameter Set We need to define variables, parameters that use those variables (in this case there are only intercepts -- and one covariate per parameter -- so the variables are effectively the same as the parameters), and a parameter set that collects all of that information into one object. alpha_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 0 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) beta_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 1 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) p_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 2 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) alpha = Parameter ( 'alpha' , link_fun = np . exp , variables = [ alpha_fe ]) beta = Parameter ( 'beta' , link_fun = lambda x : x , variables = [ beta_fe ]) p = Parameter ( 'p' , link_fun = lambda x : x , variables = [ p_fe ]) params = ParameterSet ([ alpha , beta , p ]) Models and Solvers We now need a Model that contains our parameter set and also which curve function we want to fit, and the loss function for the optimization. We also need to define a Solver that will actually perform the optimization. Finally, we will fit the solver to our simulated data. model = CoreModel ( param_set = params , curve_fun = ln_gaussian_pdf , loss_fun = normal_loss ) solver = ScipyOpt () Use the Prior Initializer The purpose of using the prior initializer is that it gets smarter priors for later on. Therefore, there are many different things that one could do to get good priors based on subsets of the data. Each of these types is implemented in a separate PriorInitializerComponent . Here we will use the BetaPrior prior initializer component; the goal is to estimate the mean and variance of the random effects and then set that as the fixed effects prior variance for a new parameter set that can be used in later model fits. Even though we only have 10 groups, our random effect variance should get pretty close to our simulated value. The BetaPrior() will update the fixed effects Gaussian prior for the beta parameter to what is estimated based on a joint model fit with random effects. # Instantiate a PriorInitializer with one component pi = PriorInitializer ( prior_initializer_components = [ BetaPrior ()]) # Create a new parameter set from running the initialization new_param_set = pi . initialize ( data = data , model_prototype = model , solver_prototype = solver ) # Make sure that the estimated variance is close to the simulated variance np . testing . assert_almost_equal ( new_param_set . fe_gprior [ 1 ], [ fe_mean [ 1 ], re_var [ 1 ]], decimal = 1 ) print ( 'prior_initializer.py: OK' )","title":"prior_initializer_xam"},{"location":"extract_md/prior_initializer_xam/#example-of-prior-initializer","text":"","title":"Example of Prior Initializer"},{"location":"extract_md/prior_initializer_xam/#function-documentation","text":"See PriorInitializer for function documentation.","title":"Function Documentation"},{"location":"extract_md/prior_initializer_xam/#example","text":"In this example, we will simulate data from the curvefit.core.functions.ln_gaussian_pdf with n_groups different groups random effects, and fit a PriorInitializer to these groups to get a new ParameterSet to be used for fitting other groups. The idea is that if you have more complete time series' for some groups, then you could first fit a model(s) to these groups alone, extract information about fixed effects means, random effects variances, etc., and then use that information to set better priors for the groups that have less complete time series'. This example shows how to generate a new ParameterSet using PriorInitializer that could then be used to fit other models. First, we need to import all of the packages and modules that we will use: import numpy as np import pandas as pd from curvefit.core.functions import ln_gaussian_pdf , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import ParameterSet , Parameter , Variable from curvefit.initializer.initializer import PriorInitializer from curvefit.initializer.initializer_component import BetaPrior from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt","title":"Example"},{"location":"extract_md/prior_initializer_xam/#simulate-data","text":"Now we will set the simulation parameters. We will simulate 10 groups with 10 time points each, and their parameters are in the order alpha , beta , and p for the ln_gaussian_pdf functional form. The random effects variance will be used to simulate parameters from the mean alpha , beta , and p for each group. # simulation parameters np . random . seed ( 10 ) time = np . arange ( 10 ) n_groups = 10 fe_mean = np . array ([ 1 , 3. , 1. ]) re_var = np . array ([ 0.01 , 0.1 , 0.1 ]) # fe_gprior and re_gprior for the solver fe_gprior = [ 0. , np . inf ] re_gprior = [ 0. , np . inf ] # how much noise to add to the data data_noise = 1. Define a function to simulate data based on these parameters, and create simulated data. def simulate_data (): re_mean = np . random . normal ( fe_mean , re_var , size = ( n_groups , len ( fe_mean ))) group_names = [ f \"group_ { i } \" for i in range ( n_groups )] groups = [] for i in range ( n_groups ): group_sim = ln_gaussian_pdf ( time , re_mean [ i , :]) + np . random . normal ( 0 , data_noise , size = len ( time )) groups . append ( pd . DataFrame ({ 'group_name' : group_names [ i ], 'obs' : group_sim , 'time' : time })) group_data = pd . concat ( groups ) group_data [ 'intercept' ] = 1 return group_data df = simulate_data () # Use the `Data` object to store information about what the columns represent data = Data ( df = df , col_t = 'time' , col_obs = 'obs' , col_covs = [ 'intercept' ], obs_space = ln_gaussian_pdf , col_group = 'group_name' , obs_se_func = lambda x : 1 )","title":"Simulate Data"},{"location":"extract_md/prior_initializer_xam/#create-a-parameter-set","text":"We need to define variables, parameters that use those variables (in this case there are only intercepts -- and one covariate per parameter -- so the variables are effectively the same as the parameters), and a parameter set that collects all of that information into one object. alpha_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 0 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) beta_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 1 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) p_fe = Variable ( covariate = 'intercept' , var_link_fun = lambda x : x , fe_init = fe_mean [ 2 ], re_init = 0. , fe_gprior = fe_gprior , re_gprior = re_gprior ) alpha = Parameter ( 'alpha' , link_fun = np . exp , variables = [ alpha_fe ]) beta = Parameter ( 'beta' , link_fun = lambda x : x , variables = [ beta_fe ]) p = Parameter ( 'p' , link_fun = lambda x : x , variables = [ p_fe ]) params = ParameterSet ([ alpha , beta , p ])","title":"Create a Parameter Set"},{"location":"extract_md/prior_initializer_xam/#models-and-solvers","text":"We now need a Model that contains our parameter set and also which curve function we want to fit, and the loss function for the optimization. We also need to define a Solver that will actually perform the optimization. Finally, we will fit the solver to our simulated data. model = CoreModel ( param_set = params , curve_fun = ln_gaussian_pdf , loss_fun = normal_loss ) solver = ScipyOpt ()","title":"Models and Solvers"},{"location":"extract_md/prior_initializer_xam/#use-the-prior-initializer","text":"The purpose of using the prior initializer is that it gets smarter priors for later on. Therefore, there are many different things that one could do to get good priors based on subsets of the data. Each of these types is implemented in a separate PriorInitializerComponent . Here we will use the BetaPrior prior initializer component; the goal is to estimate the mean and variance of the random effects and then set that as the fixed effects prior variance for a new parameter set that can be used in later model fits. Even though we only have 10 groups, our random effect variance should get pretty close to our simulated value. The BetaPrior() will update the fixed effects Gaussian prior for the beta parameter to what is estimated based on a joint model fit with random effects. # Instantiate a PriorInitializer with one component pi = PriorInitializer ( prior_initializer_components = [ BetaPrior ()]) # Create a new parameter set from running the initialization new_param_set = pi . initialize ( data = data , model_prototype = model , solver_prototype = solver ) # Make sure that the estimated variance is close to the simulated variance np . testing . assert_almost_equal ( new_param_set . fe_gprior [ 1 ], [ fe_mean [ 1 ], re_var [ 1 ]], decimal = 1 ) print ( 'prior_initializer.py: OK' )","title":"Use the Prior Initializer"},{"location":"extract_md/random_effect_xam/","text":"Getting Started Using CurveFit Generalized Logistic Model The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters. Fixed and Random Effects We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\log \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned} Covariates The constant one is the only covariate in this example. Simulated data Problem Settings The following settings are used to simulate the data and check that the solution is correct: import numpy n_time = 5 # number of time points used in the simulation n_group = 2 # number of groups rel_tol = 1e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = numpy . array ([ numpy . exp ( 2.0 ), - 1.0 , + 1.0 ]) / b_true [ 0 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ numpy . log ( 0.1 ), - 0.2 , + 0.2 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero. Time Grid A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 . Measurement values We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise. Prior We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect. Example Source Code # ------------------------------------------------------------------------- import pandas from curvefit.core.functions import expit , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # true values of parameters alpha_true = numpy . log ( a_true [ 0 ] + numpy . array ( a_true [ 1 :]) ) beta_true = b_true [ 0 ] + numpy . array ( b_true [ 1 :]) p_true = numpy . exp ( phi_true [ 0 ] + numpy . array ( phi_true [ 1 :]) ) # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for i in range ( n_group ) : group_i = 'group_' + str ( i ) alpha_i = alpha_true [ i ] beta_i = beta_true [ i ] p_i = p_true [ i ] y_i = expit ( time_grid , numpy . array ([ alpha_i , beta_i , p_i ])) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_i ) data_group += n_time * [ group_i ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = num_params * [ 'constant_one' ], col_group = 'data_group' , obs_space = expit , col_obs_se = 'measurement_std' ) a_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = a_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( a_true [ 0 ]) / 100.0 , fe_gprior = [ a_true [ 0 ], abs ( a_true [ 0 ]) / 100.0 ], fe_bounds = [ 0.0 , numpy . inf ], re_bounds = [ - 2.0 , 2.0 ] ) b_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = b_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( b_true [ 0 ]) / 100.0 , fe_gprior = [ b_true [ 0 ], abs ( b_true [ 0 ]) / 100.0 ], fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ - numpy . inf , numpy . inf ] ) phi_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = phi_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( phi_true [ 0 ]) / 100.0 , fe_gprior = [ phi_true [ 0 ], abs ( phi_true [ 0 ]) / 100.0 ], fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ - numpy . inf , numpy . inf ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = numpy . log , variables = [ a_intercept ] ) beta = Parameter ( param_name = 'beta' , link_fun = lambda x : x , variables = [ b_intercept ] ) p = Parameter ( param_name = 'p' , link_fun = numpy . exp , variables = [ phi_intercept ] ) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'disp' : 0 , 'maxiter' : 300 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } model = CoreModel ( param_set = parameters , curve_fun = expit , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = False ) if not solver . success : print ( solver . status ) params_true = [ alpha_true , beta_true , p_true ] for k in range ( 3 ): est = params_estimate [ k ] truth = params_true [ k ] rel_error = est / truth - 1.0 # for i in range(n_group) : # print(est[i], truth[i], rel_error[i]) assert numpy . allclose ( params_estimate [ i ], params_true [ i ], rtol = rel_tol ) print ( 'random_effect.py: OK' )","title":"random_effect_xam"},{"location":"extract_md/random_effect_xam/#getting-started-using-curvefit","text":"","title":"Getting Started Using CurveFit"},{"location":"extract_md/random_effect_xam/#generalized-logistic-model","text":"The model for the mean of the data for this example is one of the following: f(t; \\alpha, \\beta, p) = \\frac{p}{1 + \\exp [ -\\alpha(t - \\beta) ]} where \\alpha , \\beta , and p are unknown parameters.","title":"Generalized Logistic Model"},{"location":"extract_md/random_effect_xam/#fixed-and-random-effects","text":"We use the notation a_j , b_j and \\phi_j for the fixed effects, j = 0 , and the random effects j = 1 , \\ldots , n_G . For this example, the link functions, that maps from the fixed and random effects to the parameters, are \\begin{aligned} \\begin{aligned} \\alpha_j & = \\log \\left( a_0 + a_j \\right) \\\\ \\beta_j & = b_0 + b_j \\\\ p_j & = \\exp \\left( \\phi_0 + \\phi_j \\right) \\\\ \\end{aligned} \\end{aligned}","title":"Fixed and Random Effects"},{"location":"extract_md/random_effect_xam/#covariates","text":"The constant one is the only covariate in this example.","title":"Covariates"},{"location":"extract_md/random_effect_xam/#simulated-data","text":"","title":"Simulated data"},{"location":"extract_md/random_effect_xam/#problem-settings","text":"The following settings are used to simulate the data and check that the solution is correct: import numpy n_time = 5 # number of time points used in the simulation n_group = 2 # number of groups rel_tol = 1e-4 # relative tolerance used to check optimal solution # simulation values used for b_0, ..., b_4 b_true = [ 20.0 , - 2.0 , + 2.0 ] # simulation values used for a_0, ..., a_4 a_true = numpy . array ([ numpy . exp ( 2.0 ), - 1.0 , + 1.0 ]) / b_true [ 0 ] # simulation values used for phi_0, ..., phi_4 phi_true = [ numpy . log ( 0.1 ), - 0.2 , + 0.2 ] The fixed effects are initialized to be their true values divided by three. The random effects are initialized to be zero.","title":"Problem Settings"},{"location":"extract_md/random_effect_xam/#time-grid","text":"A grid of n_time points in time, t_i , where t_i = b_0 / ( n_t - 1 ) where n_t is the number of time points. The minimum value for this grid is zero and its maximum is b_0 .","title":"Time Grid"},{"location":"extract_md/random_effect_xam/#measurement-values","text":"We simulate data, y_{i,j} , with no noise at each of the time points. To be specific, for i = 0 , \\ldots , n_t - 1 , j = 1 , \\ldots , n_G y_{i,j} = f( t_i , \\alpha_j , \\beta_j , p_j ) There is no noise in this simulated data, but when we do the fitting, we model each data point as having noise.","title":"Measurement values"},{"location":"extract_md/random_effect_xam/#prior","text":"We want our result to fit the noiseless data perfectly, but there is only data for the random effects. We add a prior on the fixed effects to stabilize the estimation procedure. (It would be better if we could specify that the sum of the random effects corresponding to a fixed effect was zero.) Each prior is specified by a mean, equal to the true value, and a standard deviation, equal to 1/100 times the true value. The mean must be the true value for the optimal fit to be perfect.","title":"Prior"},{"location":"extract_md/random_effect_xam/#example-source-code","text":"# ------------------------------------------------------------------------- import pandas from curvefit.core.functions import expit , normal_loss from curvefit.core.data import Data from curvefit.core.parameter import Variable , Parameter , ParameterSet from curvefit.models.core_model import CoreModel from curvefit.solvers.solvers import ScipyOpt # number of parameters, fixed effects, random effects num_params = 3 num_fe = 3 num_re = num_fe * n_group # true values of parameters alpha_true = numpy . log ( a_true [ 0 ] + numpy . array ( a_true [ 1 :]) ) beta_true = b_true [ 0 ] + numpy . array ( b_true [ 1 :]) p_true = numpy . exp ( phi_true [ 0 ] + numpy . array ( phi_true [ 1 :]) ) # ----------------------------------------------------------------------- # data_frame num_data = n_time * n_group time_grid = numpy . array ( range ( n_time )) * b_true [ 0 ] / ( n_time - 1 ) independent_var = numpy . zeros ( 0 , dtype = float ) measurement_value = numpy . zeros ( 0 , dtype = float ) data_group = list () for i in range ( n_group ) : group_i = 'group_' + str ( i ) alpha_i = alpha_true [ i ] beta_i = beta_true [ i ] p_i = p_true [ i ] y_i = expit ( time_grid , numpy . array ([ alpha_i , beta_i , p_i ])) independent_var = numpy . append ( independent_var , time_grid ) measurement_value = numpy . append ( measurement_value , y_i ) data_group += n_time * [ group_i ] constant_one = num_data * [ 1.0 ] measurement_std = num_data * [ 0.1 ] data_dict = { 'independent_var' : independent_var , 'measurement_value' : measurement_value , 'measurement_std' : measurement_std , 'constant_one' : constant_one , 'data_group' : data_group , } data_frame = pandas . DataFrame ( data_dict ) # ------------------------------------------------------------------------ # curve_model data = Data ( df = data_frame , col_t = 'independent_var' , col_obs = 'measurement_value' , col_covs = num_params * [ 'constant_one' ], col_group = 'data_group' , obs_space = expit , col_obs_se = 'measurement_std' ) a_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = a_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( a_true [ 0 ]) / 100.0 , fe_gprior = [ a_true [ 0 ], abs ( a_true [ 0 ]) / 100.0 ], fe_bounds = [ 0.0 , numpy . inf ], re_bounds = [ - 2.0 , 2.0 ] ) b_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = b_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( b_true [ 0 ]) / 100.0 , fe_gprior = [ b_true [ 0 ], abs ( b_true [ 0 ]) / 100.0 ], fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ - numpy . inf , numpy . inf ] ) phi_intercept = Variable ( covariate = 'constant_one' , var_link_fun = lambda x : x , fe_init = phi_true [ 0 ] / 3.0 , re_init = 0.0 , re_zero_sum_std = abs ( phi_true [ 0 ]) / 100.0 , fe_gprior = [ phi_true [ 0 ], abs ( phi_true [ 0 ]) / 100.0 ], fe_bounds = [ - numpy . inf , numpy . inf ], re_bounds = [ - numpy . inf , numpy . inf ] ) alpha = Parameter ( param_name = 'alpha' , link_fun = numpy . log , variables = [ a_intercept ] ) beta = Parameter ( param_name = 'beta' , link_fun = lambda x : x , variables = [ b_intercept ] ) p = Parameter ( param_name = 'p' , link_fun = numpy . exp , variables = [ phi_intercept ] ) parameters = ParameterSet ([ alpha , beta , p ]) optimizer_options = { 'disp' : 0 , 'maxiter' : 300 , 'ftol' : 1e-8 , 'gtol' : 1e-8 , } model = CoreModel ( param_set = parameters , curve_fun = expit , loss_fun = normal_loss ) solver = ScipyOpt ( model ) solver . fit ( data = data . _get_df ( copy = True , return_specs = True ), options = optimizer_options ) params_estimate = model . get_params ( solver . x_opt , expand = False ) if not solver . success : print ( solver . status ) params_true = [ alpha_true , beta_true , p_true ] for k in range ( 3 ): est = params_estimate [ k ] truth = params_true [ k ] rel_error = est / truth - 1.0 # for i in range(n_group) : # print(est[i], truth[i], rel_error[i]) assert numpy . allclose ( params_estimate [ i ], params_true [ i ], rtol = rel_tol ) print ( 'random_effect.py: OK' )","title":"Example Source Code"},{"location":"extract_md/sizes_to_indices/","text":"curvefit.core.utils.sizes_to_indices Converting sizes to corresponding indices. Syntax indices = curvefit.sizes_to_indices(sizes) Arguments sizes (iterable) : The argument sizes is an iterable object with integer values. The i-th value in sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order. Returns indices : The return value indices is a list of one dimensional numpy arrays. The value indices[i] has length equal to the i-th size. It starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of indices[i] are monotone and increase by one between elements. Example sizes_to_indices_xam","title":"sizes_to_indices"},{"location":"extract_md/sizes_to_indices/#curvefitcoreutilssizes_to_indices","text":"","title":"curvefit.core.utils.sizes_to_indices"},{"location":"extract_md/sizes_to_indices/#converting-sizes-to-corresponding-indices","text":"","title":"Converting sizes to corresponding indices."},{"location":"extract_md/sizes_to_indices/#syntax","text":"indices = curvefit.sizes_to_indices(sizes)","title":"Syntax"},{"location":"extract_md/sizes_to_indices/#arguments","text":"sizes (iterable) : The argument sizes is an iterable object with integer values. The i-th value in sizes[i] is the number of elements in the i-th subvector of a larger total vector that contains the subvectors in order.","title":"Arguments"},{"location":"extract_md/sizes_to_indices/#returns","text":"indices : The return value indices is a list of one dimensional numpy arrays. The value indices[i] has length equal to the i-th size. It starts (ends) with the index in the total vector of the first (last) element of the i-th subvector. The elements of indices[i] are monotone and increase by one between elements.","title":"Returns"},{"location":"extract_md/sizes_to_indices/#example","text":"sizes_to_indices_xam","title":"Example"},{"location":"extract_md/sizes_to_indices_xam/","text":"Example and Test of sizes_to_indices Function Documentation size_to_indices Example Source Code import numpy from curvefit.core.utils import sizes_to_indices sizes = [ 2 , 4 , 3 ] indices = sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ])) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ])) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ])) print ( 'sizes_to_indices.py: OK' )","title":"sizes_to_indices_xam"},{"location":"extract_md/sizes_to_indices_xam/#example-and-test-of-sizes_to_indices","text":"","title":"Example and Test of sizes_to_indices"},{"location":"extract_md/sizes_to_indices_xam/#function-documentation","text":"size_to_indices","title":"Function Documentation"},{"location":"extract_md/sizes_to_indices_xam/#example-source-code","text":"import numpy from curvefit.core.utils import sizes_to_indices sizes = [ 2 , 4 , 3 ] indices = sizes_to_indices ( sizes ) assert all ( indices [ 0 ] == numpy . array ([ 0 , 1 ])) assert all ( indices [ 1 ] == numpy . array ([ 2 , 3 , 4 , 5 ])) assert all ( indices [ 2 ] == numpy . array ([ 6 , 7 , 8 ])) print ( 'sizes_to_indices.py: OK' )","title":"Example Source Code"},{"location":"extract_md/st_loss/","text":"Student's t Loss Function Syntax loss = curvefit.core.loss_fun.st_loss(r, nu = 1.0) t is a numpy vector of residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values. nu is the number of degrees of freedom in the t distribution \\nu . This can be a float or a_double value. Distribution The student's t-distribution is f(r) = ( 1 + r^2 / \\nu )^{- (\\nu + 1) / 2 } \\Gamma[ ( \\nu + 1) / 2 ] / [ \\sqrt{ \\nu \\pi } \\Gamma( \\nu / 2 ) ] where \\nu is the number of degrees of freedom and \\Gamma is the gamma function. Negative log Taking the negative log of the distribution function we get - \\log [ f(r) ] = \\log ( 1 + r^2 / \\nu ) (\\nu + 1) / 2 + c where c is constant w.r.t. r . loss The return value loss is a scalar equal to \\frac{\\nu + 1}{2} \\sum_{i=0}^{n-1} \\log( 1 + r_i^2 / \\nu ) Example loss_xam","title":"st_loss"},{"location":"extract_md/st_loss/#students-t-loss-function","text":"","title":"Student's t Loss Function"},{"location":"extract_md/st_loss/#syntax","text":"loss = curvefit.core.loss_fun.st_loss(r, nu = 1.0)","title":"Syntax"},{"location":"extract_md/st_loss/#t","text":"is a numpy vector of residual values. We use n for the length of the vector. The elements of this vector can be float or a_double values.","title":"t"},{"location":"extract_md/st_loss/#nu","text":"is the number of degrees of freedom in the t distribution \\nu . This can be a float or a_double value.","title":"nu"},{"location":"extract_md/st_loss/#distribution","text":"The student's t-distribution is f(r) = ( 1 + r^2 / \\nu )^{- (\\nu + 1) / 2 } \\Gamma[ ( \\nu + 1) / 2 ] / [ \\sqrt{ \\nu \\pi } \\Gamma( \\nu / 2 ) ] where \\nu is the number of degrees of freedom and \\Gamma is the gamma function.","title":"Distribution"},{"location":"extract_md/st_loss/#negative-log","text":"Taking the negative log of the distribution function we get - \\log [ f(r) ] = \\log ( 1 + r^2 / \\nu ) (\\nu + 1) / 2 + c where c is constant w.r.t. r .","title":"Negative log"},{"location":"extract_md/st_loss/#loss","text":"The return value loss is a scalar equal to \\frac{\\nu + 1}{2} \\sum_{i=0}^{n-1} \\log( 1 + r_i^2 / \\nu )","title":"loss"},{"location":"extract_md/st_loss/#example","text":"loss_xam","title":"Example"},{"location":"extract_md/unzip_x/","text":"curvefit.core.effects2params.unzip_x Extract Fixed and Random Effects from Single Vector Form Syntax fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe ) Arguments x (np.array) : a numpy vector with length equal to (num_groups + 1) * num_fe num_groups (int) : is the number of data groups num_fe (int) : is the number of fixed effects Returns fe (np.array) : is a numpy vector containing the fist num_fe elements of x . re (np.array) : is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ] Example unzip_x_xam","title":"unzip_x"},{"location":"extract_md/unzip_x/#curvefitcoreeffects2paramsunzip_x","text":"","title":"curvefit.core.effects2params.unzip_x"},{"location":"extract_md/unzip_x/#extract-fixed-and-random-effects-from-single-vector-form","text":"","title":"Extract Fixed and Random Effects from Single Vector Form"},{"location":"extract_md/unzip_x/#syntax","text":"fe , re = curvefit . core . effects2params . unzip_x ( x , num_groups , num_fe )","title":"Syntax"},{"location":"extract_md/unzip_x/#arguments","text":"x (np.array) : a numpy vector with length equal to (num_groups + 1) * num_fe num_groups (int) : is the number of data groups num_fe (int) : is the number of fixed effects","title":"Arguments"},{"location":"extract_md/unzip_x/#returns","text":"fe (np.array) : is a numpy vector containing the fist num_fe elements of x . re (np.array) : is a numpy two dimensional array with row dimension num_groups and column dimension num_fe . The i-th row of re contains the following sub-vector of x re [ i ,:] = x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ]","title":"Returns"},{"location":"extract_md/unzip_x/#example","text":"unzip_x_xam","title":"Example"},{"location":"extract_md/unzip_x_xam/","text":"Example and Test of unzip_x Function Documentation unzip_x Example Source Code import numpy from curvefit.core.effects2params import unzip_x num_groups = 2 num_fe = 3 x = numpy . array ( range (( num_groups + 1 ) * num_fe )) fe , re = unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ]) for i in range ( num_groups ): assert all ( re [ i , :] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ]) print ( 'unzip_x.py: OK' )","title":"unzip_x_xam"},{"location":"extract_md/unzip_x_xam/#example-and-test-of-unzip_x","text":"","title":"Example and Test of unzip_x"},{"location":"extract_md/unzip_x_xam/#function-documentation","text":"unzip_x","title":"Function Documentation"},{"location":"extract_md/unzip_x_xam/#example-source-code","text":"import numpy from curvefit.core.effects2params import unzip_x num_groups = 2 num_fe = 3 x = numpy . array ( range (( num_groups + 1 ) * num_fe )) fe , re = unzip_x ( x , num_groups , num_fe ) assert fe . ndim == 1 assert re . ndim == 2 assert fe . shape [ 0 ] == num_fe assert re . shape [ 0 ] == num_groups assert re . shape [ 1 ] == num_fe assert all ( fe == x [ 0 : num_fe ]) for i in range ( num_groups ): assert all ( re [ i , :] == x [( i + 1 ) * num_fe : ( i + 2 ) * num_fe ]) print ( 'unzip_x.py: OK' )","title":"Example Source Code"}]}